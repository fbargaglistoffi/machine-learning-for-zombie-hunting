---
title: "Machine Learning Analysis with Lagged Predictors"
author: "Falco J. Bargagli-Stoffi, Massimo Riccaboni, Armando Rungi"
date: "23/2/2020"
output:
  pdf_document:
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_knit$set(root.dir = 'G:\\Il mio Drive\\Research\\Italian Firms\\Zombie Hunting New Data')
# knitr::opts_knit$set(root.dir = 'H:\\.shortcut-targets-by-id\\1keYb51HXkcQwzkU2kBgbnguQaqUy3umX\\Zombie Hunting New Data')
knitr::opts_knit$set(root.dir = '/home/fabio.incerti/Desktop/Review Falco')
rm(list=ls()) # to clean the memeory
```

# Introduction

This \texttt{R Markdown} file reproduces the lagged machine learning analysis for the paper \textit{"Machine learning for zombie hunting. Firms' failures, financial constraints, and misallocation"} by Falco J. Bargagli-Stoffi (IMT School for Advanced Studies/KU Leuven), Massimo Riccaboni (IMT School for Advanced Studies) and Armando Rungi (IMT School for Advanced Studies). 

## R Markdown

This is an \texttt{R Markdown} document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using \texttt{R Markdown} see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded \texttt{R} code chunks within the document. You can embed an R code chunks like the following.

## Packages Upload

The following packages and functions are the ones used for the analyses performed in the \texttt{R} code. The \texttt{functions.R} file contains the functions \texttt{F1\_score}, \texttt{balanced\_accuracy}, \texttt{model\_compare} and \texttt{DtD} that were developed to reproduce the following analyses. 

```{r warning=FALSE, message = FALSE}
memory.limit(size=1000000)
options(java.parameters = "-Xmx1000g")
library(rJava)
library(bartMachine)
library(haven)
library(plyr)
library(dplyr)
library(PRROC)
library(rpart)
library(party)
library(finalfit)
library(caret)
library(Amelia)
library(Hmisc)
library(PresenceAbsence)
#library(devtools)
library(SuperLearner)
library(Metrics)
library(pROC)
library(Hmisc)
library(GGally)
library(xgboost)
library(tictoc)
library(corrplot)
library(dbarts)
library(tmle)
library(mice)
library(DMwR2)
source('functions.R')
```

## Data Upload

In the following chunks of code we upload the data, we initialize the main variables used in the analysis and we restrict the sample to the Italian firms.

```{r}
data <- read_dta("analysis_data_indicators.dta")
```

```{r}
names(data)[names(data) == 'GUO___BvD_ID_number'] <- 'guo'
data$control <- ifelse(data$guo=="", 0, 1)
data$nace <- as.factor(data$nace)
data$area <- as.factor(data$area)

# Avoid this conversions: it provides zero-values to all firms 
levels(data$nace) <- floor(as.numeric(levels(data$nace))/100) 

data_italy <- data[which(data$iso=="IT"),] 
```

```{r include = FALSE}
data_spain <- data[which(data$iso=="ES"),] 
data_france <- data[which(data$iso=="FR"),] 
data_portugal <- data[which(data$iso=="PT"),] 
```

## Explorative Data Analysis (EDA)

Run the following chunks of code to produce Tables 1 in the paper.

```{r include = FALSE}
status_aggregated <- as.matrix(NA, nrow = nrow(data_italy), ncol = 1)
status_aggregated[which(data_italy$Status=="Active")] <- "Active" 
status_aggregated[which(data_italy$Status=="Active (default of payment)")] <- "Active" 
status_aggregated[which(data_italy$Status=="Active (insolvency proceedings)")] <- "Active" 
status_aggregated[which(data_italy$Status=="Active (rescue plan)")] <- "Active" 
status_aggregated[which(data_italy$Status=="Bankruptcy")] <- "Bankruptcy" 
status_aggregated[which(data_italy$Status=="Dissolved")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (bankruptcy)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (demerged)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (liquidation)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (merger or take-over)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="In liquidation")] <- "In Liquidation" 
```

Remove firms in liquidation

```{r}
print("Number of italian firms in liquidation")
nrow(data_italy[which(data_italy$Status == "In liquidation"), ])

data_italy = data_italy[which(data_italy$Status != "In liquidation"), ]
```

```{r eval = FALSE}
table_1 <- table(status_aggregated)
table_1
prop.table(table_1)
```

Run the following chunk of code to produce Table 8 in the paper.

```{r eval = FALSE}
table(data$iso)
table_2 <- table(data$iso, data$failure)
table_2
prop.table(table_2, 1)
```

Run the following chunks of code to produce Table 9 in the paper. 

```{r eval = FALSE}
icr_italy <- table(data_italy[which(data_italy$year_of_status>=2016 &
                   data_italy$year_of_incorporation<=2016),]$ICR_failure_2016)
prop.table(icr_italy)
length(which(is.na(data_italy$ICR_failure)))/nrow(data_italy)
icr_spain <- table(data_spain[which(data_spain$year_of_status>=2016 &
                   data_spain$year_of_incorporation<=2016),]$ICR_failure)
prop.table(icr_spain)
icr_portugal <- table(data_portugal[which(data_portugal$year_of_status>=2016 &
                   data_portugal$year_of_incorporation<=2016),]$ICR_failure)
prop.table(icr_portugal)
icr_france <- table(data_france[which(data_france$year_of_status>=2016 &
                   data_france$year_of_incorporation<=2016),]$ICR_failure)
prop.table(icr_france)
```

```{r eval = FALSE}
neg_va_italy <- table(data_italy[which(data_italy$year_of_status>=2016 &
                      data_italy$year_of_incorporation<=2016),]$NEG_VA)
prop.table(neg_va_italy)
length(which(is.na(data_italy$NEG_VA)))/nrow(data_italy)
neg_va_spain <- table(data_spain[which(data_spain$year_of_status>=2016 &
                   data_spain$year_of_incorporation<=2016),]$NEG_VA)
prop.table(neg_va_spain)
neg_va_portugal <- table(data_portugal[which(data_portugal$year_of_status>=2016 &
                   data_portugal$year_of_incorporation<=2016),]$NEG_VA)
prop.table(neg_va_portugal)
neg_va_france <- table(data_france[which(data_france$year_of_status>=2016 &
                   data_france$year_of_incorporation<=2016),]$NEG_VA)
prop.table(neg_va_france)
```

```{r eval = FALSE}
interest_diff_italy <- table(data_italy[which(data_italy$year_of_status>=2016 &
                       data_italy$year_of_incorporation<=2016),]$interest_diff)
prop.table(interest_diff_italy)
length(which(is.na(data_italy$interest_diff)))/nrow(data_italy)
interest_diff_spain <- table(data_spain[which(data_spain$year_of_status>=2016 &
                   data_spain$year_of_incorporation<=2016),]$interest_diff)
prop.table(interest_diff_spain)
interest_diff_portugal <- table(data_portugal[which(data_portugal$year_of_status>=2016 &
                   data_portugal$year_of_incorporation<=2016),]$interest_diff)
prop.table(interest_diff_portugal)
interest_diff_france <- table(data_france[which(data_france$year_of_status>=2016 &
                   data_france$year_of_incorporation<=2016),]$interest_diff)
prop.table(interest_diff_france)
```

```{r eval = FALSE}
profitability_italy <- table(data_italy[which(data_italy$year_of_status>=2016 &
                       data_italy$year_of_incorporation<=2016),]$profitability)
prop.table(profitability_italy)
length(which(is.na(data_italy$profitability)))/nrow(data_italy)
profitability_spain <- table(data_spain[which(data_spain$year_of_status>=2016 &
                   data_spain$year_of_incorporation<=2016),]$profitability)
prop.table(profitability_spain)
profitability_portugal <- table(data_portugal[which(data_portugal$year_of_status>=2016 &
                   data_portugal$year_of_incorporation<=2016),]$profitability)
prop.table(profitability_portugal)
profitability_france <- table(data_france[which(data_france$year_of_status>=2016 &
                   data_france$year_of_incorporation<=2016),]$profitability)
prop.table(profitability_france)
```

```{r eval = FALSE}
misallocated_fixed_italy <- table(data_italy[which(data_italy$year_of_status>=2016 &
                            data_italy$year_of_incorporation<=2016),]$misallocated_fixed)
prop.table(misallocated_fixed_italy)
misallocated_fixed_spain <- table(data_spain[which(data_spain$year_of_status>=2016 &
                   data_spain$year_of_incorporation<=2016),]$misallocated_fixed)
prop.table(misallocated_fixed_spain)
misallocated_fixed_portugal <- table(data_portugal[which(data_portugal$year_of_status>=2016 &
                   data_portugal$year_of_incorporation<=2016),]$misallocated_fixed)
prop.table(misallocated_fixed_portugal)
misallocated_fixed_france <- table(data_france[which(data_france$year_of_status>=2016 &
                   data_france$year_of_incorporation<=2016),]$misallocated_fixed)
prop.table(misallocated_fixed_france)
```

Run the following code to explore the missingness patterns in the variables.

```{r eval=FALSE}
# Missingness in the variables
sapply(data_italy,function(x) sum(is.na(x)))
```

Exclude the highly missing variables:  \textit{labour\_product, retained\_earnings, firm\_value, tax\_payables, pension\_payables, pension\_tax\_debts} (above 200,000 missing: +65\% missing).

Run the following code to reproduce the "missingness maps" in Figures 1 and 2 of the paper.

```{r eval = FALSE}
raw_variables <- c("failure", "iso", "control", "Number_of_patents",
                   "Number_of_trademarks", "conscode", "nace",
                   "wage_bill","shareholders_funds", "added_value",
                   "cash_flow", "ebitda", "fin_rev", "liquidity_ratio",
                   "total_assets", "depr", "long_term_debt", "employees",
                   "materials", "loans", "fixed_assets", "tax",
                   "current_liabilities", "current_assets", 
                   "fin_expenses", "int_paid", "solvency_ratio",
                   "net_income", "revenue", "int_fixed_assets") 
raw_data_missing <- data_italy[raw_variables]
set.seed(2020)
sample_10000 <- sample(nrow(raw_data_missing), 10000, replace = FALSE)
missmap(raw_data_missing[sample_10000,], main = "Missing values vs Observed",
        margins = c(8,5), x.cex = 0.8) 
```

```{r eval = FALSE, warning=FALSE}
indicators <- c("consdummy", "capital_intensity", "fin_cons100",
                "inv", "ICR_failure", "interest_diff", "NEG_VA",
                "real_SA", "Z_score", "misallocated_fixed",
                "profitability", "area", "tfp_acf", "dummy_patents",
                "dummy_trademark", "financial_sustainability",
                "liquidity_return") 
indicators_missing <- data_italy[indicators]
missmap(indicators_missing[sample_10000,],
        main = "Missing values vs Observed",
        margins = c(8,5), x.cex = 0.8)
```

In the chunk below, we run a series of Pearson tests to assess the degree of dependence between the missing values of each variable and the dependent variable.

```{r eval = FALSE, warning=FALSE}
# ICR
miss_icr <- ifelse(is.na(data_italy$ICR_failure), 1, 0)
chisq.test(miss_icr, data_italy$failure)

# Negative AV
miss_neg_va <- ifelse(is.na(data_italy$NEG_VA), 1, 0)
chisq.test(miss_neg_va, data_italy$failure)

# FCI
miss_fin_cons <- ifelse(is.na(data_italy$fin_cons), 1, 0)
chisq.test(miss_fin_cons, data_italy$failure)

# BID
miss_bid <- ifelse(is.na(data_italy$interest_diff), 1, 0)
chisq.test(miss_bid, data_italy$failure)

# Misallocation
misallocated <- ifelse(is.na(data_italy$misallocated_fixed), 1, 0)
chisq.test(misallocated, data_italy$failure)

# TFP
tfp <- ifelse(is.na(data_italy$tfp_acf), 1, 0)
chisq.test(tfp, data_italy$failure)

# Profitability
prof <- ifelse(is.na(data_italy$profitability), 1, 0)
chisq.test(prof, data_italy$failure)

# Liquidity return
LR <- ifelse(is.na(data_italy$liquidity_return), 1, 0)
chisq.test(LR, data_italy$failure)

# Solvency
solvency <- ifelse(is.na(data_italy$solvency_ratio), 1, 0)
chisq.test(solvency, data_italy$failure)

# Capital intensity
cap_int <- ifelse(is.na(data_italy$capital_intensity), 1, 0)
chisq.test(cap_int, data_italy$failure)

# Labour productivity
lab_prod <- ifelse(is.na(data_italy$labour_product), 1, 0)
chisq.test(lab_prod, data_italy$failure)

# Size-Age
size_age <- ifelse(is.na(data_italy$real_SA), 1, 0)
chisq.test(size_age, data_italy$failure)

# Financial sustainability
fin_sust <- ifelse(is.na(data_italy$financial_sustainability), 1, 0)
chisq.test(fin_sust, data_italy$failure)

# Liquidity
liquidity <- ifelse(is.na(data_italy$liquidity_ratio), 1, 0)
chisq.test(liquidity, data_italy$failure)


# Uncomment for latex table
#latex(summary(failure ~ liquidity,
#              data=data_italy,
#              method="reverse" ,test=TRUE), exclude1=FALSE)
```


# Machine Learning Analysis

## Data Inizialization

Select the lagged variables.

```{r warning=FALSE}
# Temporarily remove "nace" from predictors: it causes problem in logit

lagged_variables <- c("failure", "iso", "control",
                      "shareholders_funds", "added_value",
                      "cash_flow", "ebitda", "fin_rev",
                      "liquidity_ratio", "total_assets",
                      "depr", "long_term_debt", "employees",
                      "materials", "loans", "wage_bill",
                      "tfp_acf", "fixed_assets", "tax",
                      "current_liabilities", "current_assets",
                      "fin_expenses", "int_paid",
                      "solvency_ratio", "net_income",
                      "revenue", "consdummy", "capital_intensity",
                      "fin_cons100", "inv", "ICR_failure",
                      "interest_diff", "NEG_VA", "real_SA",
                      "Z_score", "misallocated_fixed",
                      "profitability", "area", "dummy_patents",
                      "dummy_trademark","financial_sustainability",
                      "liquidity_return", "int_fixed_assets")
data_lagged <- data_italy[lagged_variables]
```

Select the predictors.

```{r}
#nace, area

predictors <- c("control", "shareholders_funds","nace",
                "added_value", "cash_flow", "ebitda",
                "fin_rev", "liquidity_ratio", "total_assets",
                "depr", "long_term_debt", "employees",
                "materials", "loans", "wage_bill", "tfp_acf",
                "fixed_assets", "tax", "current_liabilities",
                "current_assets", "fin_expenses", "int_paid",
                "solvency_ratio", "net_income", "revenue",
                "consdummy", "capital_intensity", "fin_cons100",
                "inv", "ICR_failure", "interest_diff", "NEG_VA",
                "real_SA", "misallocated_fixed", "profitability",
                "area", "dummy_patents", "dummy_trademark",
                "financial_sustainability", "liquidity_return",
                "int_fixed_assets")
formula <- as.formula(paste("as.factor(failure) ~",
                            paste(predictors, collapse="+")))
```

## Encoding missingness
```{r}
data_encoded = data_italy[c("failure", predictors)]

# Create the dataframe of missing values indicators (only for variables with NaN)

# missing = data.frame(ifelse(is.na(data_encoded[ , which(colSums(is.na(data_encoded)) != 0)]) == TRUE, 1, 0))

prova = c("control", "shareholders_funds",
                "added_value", "cash_flow", "ebitda",
                "consdummy", "capital_intensity", "fin_cons100",
                "inv", "ICR_failure", "interest_diff", "NEG_VA")


# Variabili problematiche con le dummies: int_fixed_assets

missing = data.frame(ifelse(is.na(data_encoded[ , prova]) == TRUE, 1, 0))
colnames(missing) = paste(colnames(missing), sep = "_", "nan_dummy")

# Replace NaN with out-of-range values
data_encoded[is.na(data_encoded)] = 10^20

# Merge the dataframes
# data_encoded = cbind(data_encoded, missing)

# Define the new model formula based on the encoded data
formula = as.formula(paste("as.factor(failure) ~", paste(colnames(data_encoded[2:length(data_encoded)]), collapse="+")))
```

Create k-fold cross-validation 
```{r}
dats = data_encoded
n.folds = 5

folds <- list() # flexible object for storing folds

fold.size <- nrow(dats)/n.folds
remain <- 1:nrow(dats) # all obs are in at the beginning

for (i in 1:n.folds){
  
    set.seed(3052)
    select <- sample(remain, fold.size, replace = FALSE)   #randomly sample “fold_size” from the ‘remaining observations’
    
    folds[[i]] <- select # store indices
    
    #write a special statement for the last fold — if there are ‘leftover points’
    
    if (i == n.folds){
      folds[[i]] <- remain
    }
    
    #update remaining indices to reflect what was taken out
    remain <- setdiff(remain, select)
    remain
  }
```

## Logit 

Check potential problem of multicollinearity: notice that our dataset has out-of-range values replacing missing values.
NB: correlations changes significantly as the imputation value changes.

In particular, if NaN are replace with 10^20 multicollineariy explodes while when are replaced with 5 is under control.
```{r}
corr_simple(data = data_encoded, sig = 0.7)
```

```{r}
time <- list()
for (i in 1:n.folds){
    
    # Segment the data  
    index <- folds[[i]]
    test <- dats[index, ]
    train <- dats[-index, ]
    
    set.seed(3052)
    tic()
    logit_cv <- glm(formula, data = train, family=binomial(link='logit'))
    time[[i]] <- toc()
    
     # Out-of-sample prediction (i.e. the i-th fold)
     fitted.prob.logit <- predict(logit_cv, newdata = test, type = 'response')
     fitted.logit <- as.numeric(fitted.prob.logit)
     
     # Separating the predictions of firms who actually failed from those who don't
     fg.logit <- fitted.logit[ test$failure == 1]
     bg.logit <- fitted.logit[ test$failure == 0]
     
     # Area under the ROC and PR curve: nota che queste due misure di performance del classificatore
     # sono THRESHOLD-FREE: misurano la performance al variare della soglia di discriminazione tra positivo e negativo 
     #  F1-Score, BACC e R-Squared invece sono threshold dependent. 
     roc_logit <- roc.curve(scores.class0 = fg.logit,
                            scores.class1 = bg.logit,
                            curve = T)
     #(roc_logit)
     
     pr_logit <- pr.curve(scores.class0 = fg.logit,
                          scores.class1 = bg.logit,
                          curve = T)
     #plot(pr_logit)
     
     
     
     # Approccio data-driven: predico che fallisci se stai sopra il 50° percentile 
     fitted.logit.integer <- ifelse(fitted.prob.logit > quantile(fitted.logit, probs = c(0.5), na.rm =TRUE), 1, 0)
     
     # F1-Score and BACC
     conf_matrix <- confusionMatrix(data = as.factor(fitted.logit.integer), 
                                    reference = as.factor(test$failure), 
                                    mode = "everything",
                                    positive = "1") 
     
     balanced_accuracy_logit <- conf_matrix[[4]][["Balanced Accuracy"]]
     f1_logit <- f1_score(fitted.logit.integer, test$failure, positive.class = "1")

     # postResample: to get RMSE, Rsquared and MAE
     accuracy_logit <- as.data.frame(rbind(postResample(as.double(fitted.logit), test$failure )))


    if(i==1) {
     logit_fit <- as.data.frame(cbind(i, roc_logit$auc, pr_logit$auc.integral, f1_logit,
                                      balanced_accuracy_logit, accuracy_logit$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
     
    } else {
     logit_fit <- rbind(logit_fit, cbind(i, roc_logit$auc, pr_logit$auc.integral, f1_logit, 
                            balanced_accuracy_logit, accuracy_logit$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
      
    }
    
    
}

colnames(logit_fit) <- c("Fold", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared", "Training time (sec.)")
rownames(logit_fit) <- NULL
logit_fit = rbind(logit_fit, c("Overall", mean(logit_fit$`Area under the ROC`), mean(logit_fit$`Area under the PR`), mean(logit_fit$`F1-Score`), mean(logit_fit$BACC), mean(logit_fit$`R squared`), round(mean(logit_fit$`Training time (sec.)`), digits = 4) ))

# Logit fit across the 5-folds and on average
logit_fit

write.csv(logit_fit, file = "logit_fit.csv")
```


## Classification Tree (CTree)
```{r}
time <- list()
for (i in 1:n.folds){
    
    # Segment the data  
    index <- folds[[i]]
    test <- dats[index, ]
    train <- dats[-index, ]
    
    set.seed(3052)
    tic()
    ctree_cv <- ctree(formula, 
                      data=train, 
                      control = ctree_control(testtype = "MonteCarlo", mincriterion = 0.90, nresample = 1000))
    time[[i]] <- toc()  

    
     # Out-of-sample prediction (i.e. the i-th fold)
     fitted.results.ctree <- as.matrix(unlist(predict(ctree_cv, newdata = test, type='prob')))
     # Take the predictions in even positions (they corresponds to the probability of failure)
     fitted.prob.ctree <- fitted.results.ctree[seq_along(fitted.results.ctree) %% 2 == 0]
     
     # Separating the predictions of firms who actually failed from those who don't
     fg.ctree <- fitted.prob.ctree[ test$failure == 1]
     bg.ctree <- fitted.prob.ctree[ test$failure == 0]
     
     # Area under the ROC and PR curve: nota che queste due misure di performance del classificatore
     # sono THRESHOLD-FREE: misurano la performance al variare della soglia di discriminazione tra positivo e negativo 
     #  F1-Score, BACC e R-Squared invece sono threshold dependent. 
     roc_ctree <- roc.curve(scores.class0 = fg.ctree,
                            scores.class1 = bg.ctree,
                            curve = T)
     #(roc_ctree)
     
     pr_ctree <- pr.curve(scores.class0 = fg.ctree,
                          scores.class1 = bg.ctree,
                          curve = T)
     #plot(pr_ctree)
     
     
     
     # Approccio data-driven: predico che fallisci se stai sopra il 50° percentile 
     fitted.ctree.integer <- ifelse(fitted.prob.ctree > quantile(fitted.prob.ctree, probs = c(0.5), na.rm =TRUE), 1, 0)
     
     # F1-Score and BACC
     conf_matrix <- confusionMatrix(data = as.factor(fitted.ctree.integer), 
                                    reference = as.factor(test$failure), 
                                    mode = "everything",
                                    positive = "1") 
     
     balanced_accuracy_ctree <- conf_matrix[[4]][["Balanced Accuracy"]]
     f1_ctree <- f1_score(fitted.ctree.integer, test$failure, positive.class = "1")

     # postResample: to get RMSE, Rsquared and MAE
     accuracy_ctree <- as.data.frame(rbind(postResample(as.double(fitted.prob.ctree), test$failure )))


    if(i==1) {
     ctree_fit <- as.data.frame(cbind(i, roc_ctree$auc, pr_ctree$auc.integral, f1_ctree,
                                      balanced_accuracy_ctree, accuracy_ctree$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
     
    } else {
     ctree_fit <- rbind(ctree_fit, cbind(i, roc_ctree$auc, pr_ctree$auc.integral, f1_ctree, 
                            balanced_accuracy_ctree, accuracy_ctree$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
      
    }
    
    
}

colnames(ctree_fit) <- c("Fold", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared", "Training time (sec.)")
rownames(ctree_fit) <- NULL
ctree_fit = rbind(ctree_fit, c("Overall", mean(ctree_fit$`Area under the ROC`), mean(ctree_fit$`Area under the PR`), mean(ctree_fit$`F1-Score`), mean(ctree_fit$BACC), mean(ctree_fit$`R squared`), round(mean(ctree_fit$`Training time (sec.)`), digits = 4) ))

# ctree fit across the 5-folds and on average
ctree_fit

write.csv(ctree_fit, file = "ctree_fit.csv")
```


## Random Forest
```{r}
time = list()
for (i in 1:n.folds){
    
    # Segment the data  
    index <- folds[[i]]
    test <- dats[index, ]
    train <- dats[-index, ]
    
    set.seed(3052)
    tic()
    rf_cv <- randomForest(formula, data = train, importance = FALSE, ntree=200)
    time[[i]]  <- toc()

     
     # Prediction out-of-sample
     fitted.prob.rf <- predict(rf_cv, newdata = test, type = "prob") 
     fitted.prob.rf <- fitted.prob.rf[,2]

     
     # Separating the predictions of firms who actually failed from those who don't
     fg.rf <- fitted.prob.rf[ test$failure == 1]
     bg.rf <- fitted.prob.rf[ test$failure == 0]
     
     # Area under the ROC and PR curve: nota che queste due misure di performance del classificatore
     # sono THRESHOLD-FREE: misurano la performance al variare della soglia di discriminazione tra positivo e negativo 
     #  F1-Score, BACC e R-Squared invece sono threshold dependent. 
     roc_rf <- roc.curve(scores.class0 = fg.rf,
                            scores.class1 = bg.rf,
                            curve = T)
     #(roc_rf)
     
     pr_rf <- pr.curve(scores.class0 = fg.rf,
                          scores.class1 = bg.rf,
                          curve = T)
     #plot(pr_rf)
     
     
     
     # Approccio data-driven: predico che fallisci se stai sopra il 50° percentile 
     fitted.rf.integer <- ifelse(fitted.prob.rf > quantile(fitted.prob.rf, probs = c(0.5), na.rm =TRUE), 1, 0)
     
     # F1-Score and BACC
     conf_matrix <- confusionMatrix(data = as.factor(fitted.rf.integer), 
                                    reference = as.factor(test$failure), 
                                    mode = "everything",
                                    positive = "1") 
     
     balanced_accuracy_rf <- conf_matrix[[4]][["Balanced Accuracy"]]
     f1_rf <- f1_score(fitted.rf.integer, test$failure, positive.class = "1")

     # postResample: to get RMSE, Rsquared and MAE
     accuracy_rf <- as.data.frame(rbind(postResample(as.double(fitted.prob.rf), test$failure )))


    if(i==1) {
     rf_fit <- as.data.frame(cbind(i, roc_rf$auc, pr_rf$auc.integral, f1_rf,
                                      balanced_accuracy_rf, accuracy_rf$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4) ))
     
    } else {
     rf_fit <- rbind(rf_fit, cbind(i, roc_rf$auc, pr_rf$auc.integral, f1_rf, 
                            balanced_accuracy_rf, accuracy_rf$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)) )
      
    }
    
    
}

colnames(rf_fit) <- c("Fold", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared", "Training time (sec.)")
rownames(rf_fit) <- NULL
rf_fit = rbind(rf_fit, c("Overall", mean(rf_fit$`Area under the ROC`), mean(rf_fit$`Area under the PR`), mean(rf_fit$`F1-Score`), mean(rf_fit$BACC), mean(rf_fit$`R squared`), round(mean(rf_fit$`Training time (sec.)`), digits = 4) ))

# rf fit across the 5-folds and on average
rf_fit

write.csv(rf_fit, file = "rf_fit.csv")
```


## XGBoost
```{r}
time = list()
for (i in 1:n.folds){
    
    # Segment the data  
    index <- folds[[i]]
    test <- dats[index, ]
    train <- dats[-index, ]
    train <- matrix(unlist(train), ncol = length(train), nrow = nrow(train))
    test <- matrix(unlist(test), ncol = length(test), nrow = nrow(test))
    
    set.seed(3052)
    tic()
    xgboost_cv <- xgboost(data = train[, 2:ncol(train)], label = train[,1], objective = "binary:logistic", nrounds = 5)
    time[[i]]  <- toc()
     
     # Prediction out-of-sample
     fitted.prob.xgboost <- predict(xgboost_cv, newdata = test[, 2:ncol(train)] ) 

     
     # Separating the predictions of firms who actually failed from those who don't
     fg.xgboost <- fitted.prob.xgboost[ test[,1] == 1]
     bg.xgboost <- fitted.prob.xgboost[ test[,1] == 0]
     
     # Area under the ROC and PR curve: nota che queste due misure di pexgboostormance del classificatore
     # sono THRESHOLD-FREE: misurano la pexgboostormance al variare della soglia di discriminazione tra positivo e negativo 
     #  F1-Score, BACC e R-Squared invece sono threshold dependent. 
     roc_xgboost <- roc.curve(scores.class0 = fg.xgboost,
                            scores.class1 = bg.xgboost,
                            curve = T)
     #(roc_xgboost)
     
     pr_xgboost <- pr.curve(scores.class0 = fg.xgboost,
                          scores.class1 = bg.xgboost,
                          curve = T)
     #plot(pr_xgboost)
     
     
     
     # Approccio data-driven: predico che fallisci se stai sopra il 50° percentile 
     fitted.xgboost.integer <- ifelse(fitted.prob.xgboost > quantile(fitted.prob.xgboost, probs = c(0.5), na.rm =TRUE), 1, 0)
     
     # F1-Score and BACC
     conf_matrix <- confusionMatrix(data = as.factor(fitted.xgboost.integer), 
                                    reference = as.factor(test[,1]), 
                                    mode = "everything",
                                    positive = "1") 
     
     balanced_accuracy_xgboost <- conf_matrix[[4]][["Balanced Accuracy"]]
     f1_xgboost <- f1_score(fitted.xgboost.integer, test[,1], positive.class = "1")

     # postResample: to get RMSE, Rsquared and MAE
     accuracy_xgboost <- as.data.frame(rbind(postResample(as.double(fitted.prob.xgboost), test[,1] )))


    if(i==1) {
     xgboost_fit <- as.data.frame(cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost,
                                      balanced_accuracy_xgboost, accuracy_xgboost$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
     
    } else {
     xgboost_fit <- rbind(xgboost_fit, cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost, 
                            balanced_accuracy_xgboost, accuracy_xgboost$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
      
    }
    
    
}

colnames(xgboost_fit) <- c("Fold", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared", "Training time (sec.)")
rownames(xgboost_fit) <- NULL
xgboost_fit = rbind(xgboost_fit, c("Overall", mean(xgboost_fit$`Area under the ROC`), mean(xgboost_fit$`Area under the PR`), mean(xgboost_fit$`F1-Score`), mean(xgboost_fit$BACC), mean(xgboost_fit$`R squared`), round(mean(xgboost_fit$`Training time (sec.)`), digits = 4) ))

# xgboost fit across the 5-folds and on average
xgboost_fit

write.csv(xgboost_fit, file = "xgboost_fit.csv")
```


## BART - without MIA -
```{r}
time <- list()
for (i in 1:n.folds){
    
      # Segment the data  
      index <- folds[[i]]
      test <- dats[index, ]
      train <- dats[-index, ]
      
      
      # Get TRAINING predictors matrix
      X <- as.data.frame(train[, 2:length(train)])
      
      # Get TRAINING outcome Vector
      y <- as.vector(train[,1])
      y <- as.factor(unlist(y, use.names = FALSE))
    
      # Train the machine 
      set.seed(3052)
      options(java.parameters = "-Xmx1000g") 
      tic()
      bart_cv<-bartMachine(X, y, use_missing_data = FALSE)
      time[[i]] <- toc() 
      
      # Get TEST predictors matrix 
      M <- as.data.frame(test[, 2:length(test)])
    
      # Get fitted values: "0" is considered the target level, for this reason we use (1 - predict)
      fitted.prob.bart <-  1 - round(predict(bart_cv, M, type='prob'), 6)    
      
     # Separating the predictions of firms who actually failed from those who don't
     fg.bart <- fitted.prob.bart[ test$failure == 1]
     bg.bart <- fitted.prob.bart[ test$failure == 0]
     
     # Area under the ROC and PR curve: nota che queste due misure di pebartormance del classificatore
     # sono THRESHOLD-FREE: misurano la pebartormance al variare della soglia di discriminazione tra positivo e negativo 
     #  F1-Score, BACC e R-Squared invece sono threshold dependent. 
     roc_bart <- roc.curve(scores.class0 = fg.bart,
                            scores.class1 = bg.bart,
                            curve = T)
     #(roc_bart)
     
     pr_bart <- pr.curve(scores.class0 = fg.bart,
                          scores.class1 = bg.bart,
                          curve = T)
     #plot(pr_bart)
     
     
     
     # Approccio data-driven: predico che fallisci se stai sopra il 50° percentile 
     fitted.bart.integer <- ifelse(fitted.prob.bart > quantile(fitted.prob.bart, probs = c(0.5), na.rm =TRUE), 1, 0)
     
     # F1-Score and BACC
     conf_matrix <- confusionMatrix(data = as.factor(fitted.bart.integer), 
                                    reference = as.factor(test$failure), 
                                    mode = "everything",
                                    positive = "1") 
     
     balanced_accuracy_bart <- conf_matrix[[4]][["Balanced Accuracy"]]
     f1_bart <- f1_score(fitted.bart.integer, test$failure, positive.class = "1")

     # postResample: to get RMSE, Rsquared and MAE
     accuracy_bart <- as.data.frame(rbind(postResample(as.double(fitted.prob.bart), test$failure )))


    if(i==1) {
     bart_fit <- as.data.frame(cbind(i, roc_bart$auc, pr_bart$auc.integral, f1_bart,
                                      balanced_accuracy_bart, accuracy_bart$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
     
    } else {
     bart_fit <- rbind(bart_fit, cbind(i, roc_bart$auc, pr_bart$auc.integral, f1_bart, 
                            balanced_accuracy_bart, accuracy_bart$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
      
    }
    
    
}

colnames(bart_fit) <- c("Fold", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared", "Training time (sec.)")
rownames(bart_fit) <- NULL
bart_fit = rbind(bart_fit, c("Overall", mean(bart_fit$`Area under the ROC`), mean(bart_fit$`Area under the PR`), mean(bart_fit$`F1-Score`), mean(bart_fit$BACC), mean(bart_fit$`R squared`), round(mean(bart_fit$`Training time (sec.)`), digits = 4) ))

# bart fit across the 5-folds and on average
bart_fit

write.csv(bart_fit, file = "bart_fit.csv")
```


## Super Learner
```{r}
time <- list()
coef <- list()

for (i in 1:n.folds){
    
      # Segment the data  
      index <- folds[[i]]
      test <- dats[index, ]
      train <- dats[-index, ]
      
      # Get TRAINING predictors matrix
      X <- as.data.frame(train[, 2:length(train)])
      
      # Get TRAINING outcome Vector
      Y <- unlist(train[,1], use.names = FALSE)
      
      # Get TEST predictors matrix 
      M <- as.data.frame(test[, 2:length(test)])
      
      tic()
    
      # V = 2 is the number of folds that the Super Learner (SL) uses to estimate the risk on future data. 
      # By default the SL uses V = 10. However, but since we do not use this measure, we reduce the computational burden setting V=2. 
      # For more information see: https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html
      SL_cv <- SuperLearner(Y = Y, 
                            X = X, 
                            SL.library = c("SL.glm", "SL.rpartPrune", "SL.randomForest", "SL.xgboost", "tmle.SL.dbarts2"), 
                            verbose = FALSE, 
                            method = "method.NNLS", 
                            family = binomial(link='logit'), 
                            cvControl = list(V = 2) )
      

      
      time[[i]] <- toc() 
    

     # Get fitted values  
     fitted <- predict(SL_cv, M, onlySL = TRUE)
     fitted.prob.sl <- fitted$pred
      
     # Separating the predictions of firms who actually failed from those who don't
     fg.sl <- fitted.prob.sl[ test$failure == 1]
     bg.sl <- fitted.prob.sl[ test$failure == 0]
     
     # Area under the ROC and PR curve: nota che queste due misure di peslormance del classificatore
     # sono THRESHOLD-FREE: misurano la peslormance al variare della soglia di discriminazione tra positivo e negativo 
     #  F1-Score, BACC e R-Squared invece sono threshold dependent. 
     roc_sl <- roc.curve(scores.class0 = fg.sl,
                            scores.class1 = bg.sl,
                            curve = T)
     #(roc_sl)
     
     pr_sl <- pr.curve(scores.class0 = fg.sl,
                          scores.class1 = bg.sl,
                          curve = T)
     #plot(pr_sl)
     
     
     
     # Approccio data-driven: predico che fallisci se stai sopra il 50° percentile 
     fitted.sl.integer <- ifelse(fitted.prob.sl > quantile(fitted.prob.sl, probs = c(0.5), na.rm =TRUE), 1, 0)
     
     # F1-Score and BACC
     conf_matrix <- confusionMatrix(data = as.factor(fitted.sl.integer), 
                                    reference = as.factor(test$failure), 
                                    mode = "everything",
                                    positive = "1") 
     
     balanced_accuracy_sl <- conf_matrix[[4]][["Balanced Accuracy"]]
     f1_sl <- f1_score(fitted.sl.integer, test$failure, positive.class = "1")

     # postResample: to get RMSE, Rsquared and MAE
     accuracy_sl <- as.data.frame(rbind(postResample(as.double(fitted.prob.sl), test$failure )))


    if(i==1) {
     sl_fit <- as.data.frame(cbind(i, roc_sl$auc, pr_sl$auc.integral, f1_sl,
                                      balanced_accuracy_sl, accuracy_sl$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
     coef[[i]] = c(i,  unname(SL_cv$coef))
     
    } else {
     sl_fit <- rbind(sl_fit, cbind(i, roc_sl$auc, pr_sl$auc.integral, f1_sl, 
                            balanced_accuracy_sl, accuracy_sl$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
     coef[[i]] = c(i,  unname(SL_cv$coef))
      
    }
    
    
}

colnames(sl_fit) <- c("Fold", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared", "Training time (sec.)")
rownames(sl_fit) <- NULL
sl_fit = rbind(sl_fit, c("Overall", mean(sl_fit$`Area under the ROC`), mean(sl_fit$`Area under the PR`), mean(sl_fit$`F1-Score`), mean(sl_fit$BACC), mean(sl_fit$`R squared`), round(mean(sl_fit$`Training time (sec.)`), digits = 4) ))

# sl fit across the 5-folds and on average
sl_fit

# Save weights given to each model, in each fold and on average
tot_coef = as.data.frame(rbind(coef[[1]], coef[[2]], coef[[3]], coef[[4]], coef[[5]]))
colnames(tot_coef) = c("Fold", "Logit", "CART", "Random Forest", "XGBoost", "BART")
tot_coef = rbind(tot_coef, c("Mean", mean(tot_coef$`Logit`), mean(tot_coef$`CART`),  mean(tot_coef$`Random Forest`),  mean(tot_coef$`XGBoost`),  mean(tot_coef$`BART`)) ) 

tot_coef

write.csv(sl_fit, file = "sl_fit.csv")
write.csv(tot_coef, file = "sl_coef.csv")

```


## BART-MIA on non-encoded missing data
```{r}
data_missing = data_italy[c("failure", predictors)]

time <- list()
for (i in 1:n.folds){
    
      # Segment the data  
      index <- folds[[i]]
      test <- data_missing[index, ]
      train <- data_missing[-index, ]
      
      
      # Get TRAINING predictors matrix
      X <- as.data.frame(train[, 2:length(train)])
      
      # Get TRAINING outcome Vector
      y <- as.vector(train[,1])
      y <- as.factor(unlist(y, use.names = FALSE))
    
      # Train the machine 
      set.seed(3052)
      options(java.parameters = "-Xmx1000g") 
      tic()
      bart_cv<-bartMachine(X, y, use_missing_data = TRUE)
      time[[i]] <- toc() 
      
      # Get TEST predictors matrix 
      M <- as.data.frame(test[, 2:length(test)])
    
      # Get fitted values: "0" is considered the target level, for this reason we use (1 - predict)
      fitted.prob.bart <-  1 - round(predict(bart_cv, M, type='prob'), 6)    
      
     # Separating the predictions of firms who actually failed from those who don't
     fg.bart <- fitted.prob.bart[ test$failure == 1]
     bg.bart <- fitted.prob.bart[ test$failure == 0]
     
     # Area under the ROC and PR curve: nota che queste due misure di pebartormance del classificatore
     # sono THRESHOLD-FREE: misurano la pebartormance al variare della soglia di discriminazione tra positivo e negativo 
     #  F1-Score, BACC e R-Squared invece sono threshold dependent. 
     roc_bart <- roc.curve(scores.class0 = fg.bart,
                            scores.class1 = bg.bart,
                            curve = T)
     #(roc_bart)
     
     pr_bart <- pr.curve(scores.class0 = fg.bart,
                          scores.class1 = bg.bart,
                          curve = T)
     #plot(pr_bart)
     
     
     
     # Approccio data-driven: predico che fallisci se stai sopra il 50° percentile 
     fitted.bart.integer <- ifelse(fitted.prob.bart > quantile(fitted.prob.bart, probs = c(0.5), na.rm =TRUE), 1, 0)
     
     # F1-Score and BACC
     conf_matrix <- confusionMatrix(data = as.factor(fitted.bart.integer), 
                                    reference = as.factor(test$failure), 
                                    mode = "everything",
                                    positive = "1") 
     
     balanced_accuracy_bart <- conf_matrix[[4]][["Balanced Accuracy"]]
     f1_bart <- f1_score(fitted.bart.integer, test$failure, positive.class = "1")

     # postResample: to get RMSE, Rsquared and MAE
     accuracy_bart <- as.data.frame(rbind(postResample(as.double(fitted.prob.bart), test$failure )))


    if(i==1) {
     bart_missing_fit <- as.data.frame(cbind(i, roc_bart$auc, pr_bart$auc.integral, f1_bart,
                                      balanced_accuracy_bart, accuracy_bart$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
     
    } else {
     bart_missing_fit <- rbind(bart_missing_fit, cbind(i, roc_bart$auc, pr_bart$auc.integral, f1_bart, 
                            balanced_accuracy_bart, accuracy_bart$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
      
    }
    
    
}

colnames(bart_missing_fit) <- c("Fold", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared", "Training time (sec.)")
rownames(bart_missing_fit) <- NULL
bart_missing_fit = rbind(bart_missing_fit, c("Overall", mean(bart_missing_fit$`Area under the ROC`), mean(bart_missing_fit$`Area under the PR`), mean(bart_missing_fit$`F1-Score`), mean(bart_missing_fit$BACC), mean(bart_missing_fit$`R squared`), round(mean(bart_missing_fit$`Training time (sec.)`), digits = 4) ))

# bart fit across the 5-folds and on average
bart_missing_fit

write.csv(bart_missing_fit, file = "bart_missing_fit.csv")
```




## Missing values imputation with CART
```{r}
data_missing = data_italy[c("failure", predictors)]

# Imputation with CART
temp = mice(data_imputed, method = "cart", m = 1, print = FALSE, remove.collinear = FALSE)

# Get imputed data
data_imputed_cart = complete(temp)
```


BART - without MIA - on imputed data with CART
```{r}
time <- list()
for (i in 1:n.folds){
    
      # Segment the data  
      index <- folds[[i]]
      test <- data_imputed_cart[index, ]
      train <- data_imputed_cart[-index, ]
      
      
      # Get TRAINING predictors matrix
      X <- as.data.frame(train[, 2:length(train)])
      
      # Get TRAINING outcome Vector
      y <- as.vector(train[,1])
      y <- as.factor(unlist(y, use.names = FALSE))
    
      # Train the machine 
      set.seed(3052)
      options(java.parameters = "-Xmx1000g") 
      tic()
      bart_cv<-bartMachine(X, y, use_missing_data = FALSE)
      time[[i]] <- toc() 
      
      # Get TEST predictors matrix 
      M <- as.data.frame(test[, 2:length(test)])
    
      # Get fitted values: "0" is considered the target level, for this reason we use (1 - predict)
      fitted.prob.bart <-  1 - round(predict(bart_cv, M, type='prob'), 6)    
      
     # Separating the predictions of firms who actually failed from those who don't
     fg.bart <- fitted.prob.bart[ test$failure == 1]
     bg.bart <- fitted.prob.bart[ test$failure == 0]
     
     # Area under the ROC and PR curve: nota che queste due misure di pebartormance del classificatore
     # sono THRESHOLD-FREE: misurano la pebartormance al variare della soglia di discriminazione tra positivo e negativo 
     #  F1-Score, BACC e R-Squared invece sono threshold dependent. 
     roc_bart <- roc.curve(scores.class0 = fg.bart,
                            scores.class1 = bg.bart,
                            curve = T)
     #(roc_bart)
     
     pr_bart <- pr.curve(scores.class0 = fg.bart,
                          scores.class1 = bg.bart,
                          curve = T)
     #plot(pr_bart)
     
     
     
     # Approccio data-driven: predico che fallisci se stai sopra il 50° percentile 
     fitted.bart.integer <- ifelse(fitted.prob.bart > quantile(fitted.prob.bart, probs = c(0.5), na.rm =TRUE), 1, 0)
     
     # F1-Score and BACC
     conf_matrix <- confusionMatrix(data = as.factor(fitted.bart.integer), 
                                    reference = as.factor(test$failure), 
                                    mode = "everything",
                                    positive = "1") 
     
     balanced_accuracy_bart <- conf_matrix[[4]][["Balanced Accuracy"]]
     f1_bart <- f1_score(fitted.bart.integer, test$failure, positive.class = "1")

     # postResample: to get RMSE, Rsquared and MAE
     accuracy_bart <- as.data.frame(rbind(postResample(as.double(fitted.prob.bart), test$failure )))


    if(i==1) {
     bart_fit <- as.data.frame(cbind(i, roc_bart$auc, pr_bart$auc.integral, f1_bart,
                                      balanced_accuracy_bart, accuracy_bart$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
     
    } else {
     bart_fit <- rbind(bart_fit, cbind(i, roc_bart$auc, pr_bart$auc.integral, f1_bart, 
                            balanced_accuracy_bart, accuracy_bart$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
      
    }
    
    
}

colnames(bart_fit) <- c("Fold", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared", "Training time (sec.)")
rownames(bart_fit) <- NULL
bart_fit = rbind(bart_fit, c("Overall", mean(bart_fit$`Area under the ROC`), mean(bart_fit$`Area under the PR`), mean(bart_fit$`F1-Score`), mean(bart_fit$BACC), mean(bart_fit$`R squared`), round(mean(bart_fit$`Training time (sec.)`), digits = 4) ))

# bart fit across the 5-folds and on average
bart_fit

write.csv(bart_fit, file = "bart_cart_imputation_fit.csv")
```


## Missing values imputation with KNN
```{r}
data_missing = data_italy[c("failure", predictors)]

# Imputation with CART
temp = knnImputation(prova, k = 10, scale = TRUE, meth = "weighAvg")

# Get imputed data
data_imputed_knn = complete(temp)
```

BART - without MIA - on imputed data with KNN
```{r}
time <- list()
for (i in 1:n.folds){
    
      # Segment the data  
      index <- folds[[i]]
      test <- data_imputed_knn[index, ]
      train <- data_imputed_knn[-index, ]
      
      
      # Get TRAINING predictors matrix
      X <- as.data.frame(train[, 2:length(train)])
      
      # Get TRAINING outcome Vector
      y <- as.vector(train[,1])
      y <- as.factor(unlist(y, use.names = FALSE))
    
      # Train the machine 
      set.seed(3052)
      options(java.parameters = "-Xmx1000g") 
      tic()
      bart_cv<-bartMachine(X, y, use_missing_data = FALSE)
      time[[i]] <- toc() 
      
      # Get TEST predictors matrix 
      M <- as.data.frame(test[, 2:length(test)])
    
      # Get fitted values: "0" is considered the target level, for this reason we use (1 - predict)
      fitted.prob.bart <-  1 - round(predict(bart_cv, M, type='prob'), 6)    
      
     # Separating the predictions of firms who actually failed from those who don't
     fg.bart <- fitted.prob.bart[ test$failure == 1]
     bg.bart <- fitted.prob.bart[ test$failure == 0]
     
     # Area under the ROC and PR curve: nota che queste due misure di pebartormance del classificatore
     # sono THRESHOLD-FREE: misurano la pebartormance al variare della soglia di discriminazione tra positivo e negativo 
     #  F1-Score, BACC e R-Squared invece sono threshold dependent. 
     roc_bart <- roc.curve(scores.class0 = fg.bart,
                            scores.class1 = bg.bart,
                            curve = T)
     #(roc_bart)
     
     pr_bart <- pr.curve(scores.class0 = fg.bart,
                          scores.class1 = bg.bart,
                          curve = T)
     #plot(pr_bart)
     
     
     
     # Approccio data-driven: predico che fallisci se stai sopra il 50° percentile 
     fitted.bart.integer <- ifelse(fitted.prob.bart > quantile(fitted.prob.bart, probs = c(0.5), na.rm =TRUE), 1, 0)
     
     # F1-Score and BACC
     conf_matrix <- confusionMatrix(data = as.factor(fitted.bart.integer), 
                                    reference = as.factor(test$failure), 
                                    mode = "everything",
                                    positive = "1") 
     
     balanced_accuracy_bart <- conf_matrix[[4]][["Balanced Accuracy"]]
     f1_bart <- f1_score(fitted.bart.integer, test$failure, positive.class = "1")

     # postResample: to get RMSE, Rsquared and MAE
     accuracy_bart <- as.data.frame(rbind(postResample(as.double(fitted.prob.bart), test$failure )))


    if(i==1) {
     bart_fit <- as.data.frame(cbind(i, roc_bart$auc, pr_bart$auc.integral, f1_bart,
                                      balanced_accuracy_bart, accuracy_bart$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
     
    } else {
     bart_fit <- rbind(bart_fit, cbind(i, roc_bart$auc, pr_bart$auc.integral, f1_bart, 
                            balanced_accuracy_bart, accuracy_bart$Rsquared, round(time[[i]]$toc- time[[i]]$tic, digits = 4)))
      
    }
    
    
}

colnames(bart_fit) <- c("Fold", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared", "Training time (sec.)")
rownames(bart_fit) <- NULL
bart_fit = rbind(bart_fit, c("Overall", mean(bart_fit$`Area under the ROC`), mean(bart_fit$`Area under the PR`), mean(bart_fit$`F1-Score`), mean(bart_fit$BACC), mean(bart_fit$`R squared`), round(mean(bart_fit$`Training time (sec.)`), digits = 4) ))

# bart fit across the 5-folds and on average
bart_fit

write.csv(bart_fit, file = "bart_knn_imputation_fit.csv")
```



---
title: "Year by year analysis"
author: "Fabio Incerti"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'H:\\.shortcut-targets-by-id\\1keYb51HXkcQwzkU2kBgbnguQaqUy3umX\\Zombie Hunting New Data')
```

Clean the environment
```{r}
rm(list=ls())
```

```{r warning=FALSE}
library(haven)
library(mlr)
library(xgboost)
library(data.table)
library(caret)
library(PRROC)
library(gtools)
library(sur)
library(Amelia)
library(dplyr)
library(tidyr)
library(naniar)
library(plotrix)
library(ggplot2)
library(fmsb)
library(reshape2)

# Server
# source('/home/fabio.incerti/Desktop/Review Falco/functions.R')

# Local
source('C:/Users/fabio/OneDrive/Documenti/GitHub/machine-learning-for-zombie-hunting/R_code/functions/functions.R')
```

Upload data
```{r}
data <- read_dta("analysis_data_indicators.dta")
```

Get Italian data
```{r}
data_italy <- data[which(data$iso=="IT"),] 
```

```{r}
names(data_italy)[names(data_italy) == 'GUO___BvD_ID_number'] <- 'guo'

data_italy$control <- ifelse(data_italy$guo=="", 0, 1)
data_italy$nace <- as.factor(data_italy$nace)
data_italy$area <- as.factor(data_italy$area)

levels(data_italy$nace) <- floor(as.numeric(levels(data_italy$nace))/100) 
```

```{r}
status_aggregated <- as.matrix(NA, nrow = nrow(data_italy), ncol = 1)
status_aggregated[which(data_italy$Status=="Active")] <- "Active" 
status_aggregated[which(data_italy$Status=="Active (default of payment)")] <- "Active" 
status_aggregated[which(data_italy$Status=="Active (insolvency proceedings)")] <- "Active" 
status_aggregated[which(data_italy$Status=="Active (rescue plan)")] <- "Active" 
status_aggregated[which(data_italy$Status=="Bankruptcy")] <- "Bankruptcy" 
status_aggregated[which(data_italy$Status=="Dissolved")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (bankruptcy)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (demerged)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (liquidation)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (merger or take-over)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="In liquidation")] <- "In Liquidation" 
```


Rename incorrectly labeled variables:
- misallocated_2008_fixed -> misallocated_fixed_2008
- revenue2008 -> revenue_2008
- depr2008 -> depr_2008                                                                            )
```{r}
for (i in c(2008:2016)) {
  colnames(data_italy) <- gsub(paste("misallocated", sep="_", i, "fixed"), paste("misallocated", sep="_", "fixed", i), colnames(data_italy))
  colnames(data_italy) <- gsub(paste("revenue", sep="", i), paste("revenue", sep="_", i), colnames(data_italy))
  colnames(data_italy) <- gsub(paste("depr", sep="", i), paste("depr", sep="_", i), colnames(data_italy))
}
```


Aggregated failure status composition
```{r}
print("Number of failures per category")
table(as.factor(status_aggregated[status_aggregated!="Active"]))
print("Failures share per category")
percent.table(x = as.factor(status_aggregated[status_aggregated!="Active"]), y = NULL)
```

Print the number of liquidations
```{r}
print("Number of firms in liquidation")
nrow(data_italy[which(data_italy$Status == "In liquidation"), ])

# Save firms in liquidation
data_italy_liq = data_italy[which(data_italy$Status == "In liquidation"), ]

# Save bankruptcy and dissolved firms
data_italy_non_liq = data_italy[which(data_italy$Status == "Bankruptcy" | data_italy$Status == "Dissolved"), ] 

# Uncomment to drop firms in liquidation
#data_italy = data_italy[which(data_italy$Status != "In liquidation"), ]
```



Construct yearly failure Indicator
```{r}
data_italy <- failure_yearly(data_italy)
```

Save data with year-by-year failures
```{r}
write_dta(data_italy, "data_failure.dta")
```


Get number of (overall) failures over time
```{r}
colSums(data_italy[which(colnames(data_italy) == "failure_2009"):which(colnames(data_italy) == "failure_2017")], na.rm = TRUE)
```

Get number of liquidations over time
```{r}
data_italy_liq <- failure_yearly(data_italy_liq)
colSums(data_italy_liq[which(colnames(data_italy_liq) == "failure_2009"):which(colnames(data_italy_liq) == "failure_2017")], na.rm = TRUE)
```



Define the relevant predictors
```{r}
# Years 2008-2016
lagged_predictors <- c("shareholders_funds",
                "added_value", "cash_flow", "ebitda",
                "fin_rev", "liquidity_ratio", "total_assets",
                "depr", "long_term_debt", "employees",
                "materials", "loans", "wage_bill", "tfp_acf",
                "fixed_assets", "tax", "current_liabilities",
                "current_assets", "fin_expenses", "int_paid",
                "solvency_ratio", "net_income", "revenue",
                "capital_intensity", "fin_cons",
                "ICR_failure", "NEG_VA",
                "real_SA", "profitability", "misallocated_fixed",
                "financial_sustainability", "liquidity_return",
                "int_fixed_assets")
dyn_predictors <- paste(lagged_predictors, sep = "_", 2008, collapse = NULL)
for (i in c(2009:2016)) {
                    dyn_predictors <- append(dyn_predictors, paste(lagged_predictors, sep = "_", i, collapse = NULL) ) 
  
}


# Years 2009-2016
outside = c("inv", "interest_diff")
dyn_outside <- paste(outside, sep = "_", 2009, collapse = NULL)
for (i in c(2010:2016)) {
                    dyn_outside <- append(dyn_outside, paste(outside, sep = "_", i, collapse = NULL) ) 
  
}

# Time-invariant
predictors <- c("control", "nace", "consdummy", "area", "dummy_patents", "dummy_trademark")
```


# Missingness patterns (Figure A.1)

```{r}
# Define new labels for ggplot
new.names <-  c(control = "Corporate control",
                miss = "Missingness",
                net_income = "Net income",
                employees = "Employees",
                tax = "Tax",
                real_SA = "Size-Age",
                fixed_assets = "Fixed assets",
                shareholders_funds = "Shareholders funds",
                revenue = "Revenues",
                tfp_acf = "TFP",
                inv = "Investement indicator",
                current_liabilities = "Current liabilities",
                current_assets = "Current assets",
                wage_bill = "Cost of employees",
                solvency_ratio = "Solvency ratio",
                total_assets = "Total assets",
                int_fixed_assets = "Intangible fixed assets",
                fin_rev = "Financial revenues",
                nace = "NACE rev. 2",
                materials = "Material costs",
                liquidity_return = "Liquidity return",
                fin_expenses = "Financial expenses",
                depr = "Depreciation",
                area = "Area",
                liquidity_ratio = "Liquidity ratio",
                ebitda = "EBITDA",
                int_paid = "Interest payments",
                financial_sustainability = "Financial sustainability",
                capital_intensity = "Capital intensity",
                loans = "Loans",
                added_value = "Added value",
                long_term_debt = "Long-term debt",
                cash_flow = "Cash flow",
                fin_cons = "Financial Constraints",
                interest_diff = "Benchmark interest difference",
                dummy_trademark = "Dummy trademarks",
                dummy_patents = "Dummy patents",
                misallocated_fixed = "Financial misallocation",
                ICR_failure = "ICR",
                profitability = "Profitability",
                consdummy = "Dummy consolidated accounts",
                NEG_VA = "Negative value added")
```


Plot missingness patterns for non-failing firms (N = 287587)
```{r}
# Replace "_" with "." for reshaping data from wide to long format (e.g. shareholders_funds_2008 --> shareholders_funds.2009)
prova = data_italy[ which(status_aggregated=="Active"), c("id", dyn_predictors)]
for (i in c(2008:2016)) {
  colnames(prova) = gsub(paste("_", sep = "", i), paste(".", sep = "", i), colnames(prova))
}

# Reshape data from wide to long format
prova_2 = reshape(data = as.data.frame(prova), 
                  varying = 2:length(prova),
                  timevar = "Year",
                  idvar = "id",
                  direction = "long",
                  sep = ".")

# Plot missingness patterns over years
gg_miss_fct(prova_2[2:length(prova_2)], Year) + 
  labs(title = "Non-failing firms", 
           x = "Years", 
           y = "Variables") +
  scale_x_discrete(limits = c(2008:2016)) +
  scale_y_discrete(labels = new.names) +
  scale_fill_continuous(type = "viridis", name = "% Missing") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.y = element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank()) 

# Uncomment for table information
#sapply(data_italy[c("id", dyn_predictors)], function(x) sum(is.na(x))/nrow(data_italy))
```


Firms in liquidation
```{r}
# Replace "_" with "." for reshaping data from wide to long format (e.g. shareholders_funds_2008 --> shareholders_funds.2009)
prova = data_italy_liq[c("id", dyn_predictors)]
for (i in c(2008:2016)) {
  colnames(prova) = gsub(paste("_", sep = "", i), paste(".", sep = "", i), colnames(prova))
}

# Reshape data from wide to long format
prova_2 = reshape(data = as.data.frame(prova), 
                  varying = 2:length(prova),
                  timevar = "Year",
                  idvar = "id",
                  direction = "long",
                  sep = ".")

# Plot missingness patterns over years
gg_miss_fct(prova_2[2:length(prova_2)], Year) + 
  labs(title = "Liquidations", 
           x = "Years", 
           y = "Variables") +
  scale_x_discrete(limits = c(2008:2016)) +
  scale_y_discrete(labels = new.names) +
  scale_fill_continuous(type = "viridis", name = "% Missing") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.y = element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank()) 

# Uncomment for table information
#sapply(data_italy_liq[c("id", dyn_predictors)], function(x) sum(is.na(x))/nrow(data_italy_liq))
```


Bankruptcies and dissolved firms
```{r}
# Replace "_" with "." for reshaping data from wide to long format (e.g. shareholders_funds_2008 --> shareholders_funds.2009)
prova = data_italy_non_liq[c("id", dyn_predictors)]
for (i in c(2008:2016)) {
  colnames(prova) = gsub(paste("_", sep = "", i), paste(".", sep = "", i), colnames(prova))
}

# Reshape data from wide to long format
prova_2 = reshape(data = as.data.frame(prova), 
                  varying = 2:length(prova),
                  timevar = "Year",
                  idvar = "id",
                  direction = "long",
                  sep = ".")

# Plot missingness patterns over years
gg_miss_fct(prova_2[2:length(prova_2)], Year) + 
  labs(title = "Bankruptcies and other dissolved firms", 
           x = "Years", 
           y = "Variables" ) +
  scale_x_discrete(limits = c(2008:2016)) +
  scale_y_discrete(labels = new.names) +
  scale_fill_continuous(type = "viridis", name = "% Missing") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.y = element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank()) 

# Uncomment for table information
#sapply(data_italy_non_liq[c("id", dyn_predictors)], function(x) sum(is.na(x))/nrow(data_italy_non_liq))
```










# Analysis for Italian Firms (2009-2017)

XGBoost requires one-hot encoding of the categorical predictors (i.e. nace and area)
```{r}
# Get one-hot encoded data.frame
dummy = dummyVars(" ~ nace + area", data = data_italy)
temp = data.frame(predict(dummy, newdata = data_italy))

# Add one-hot encoded variables to the original data
train = cbind(data_italy, temp)

# Drop the original nace and area variables
train = within(train, rm("nace", "area"))
```


Year-by-year analysis: train XGBoost with missing values for the years 2009-2017
```{r}
years <- c(2008:2016)
prob <- matrix(NA, ncol = length(years), nrow = nrow(train))

for (i in (years)){
    
    if (i == 2008) {

        # Subset data by year
        data_model <- train[which(!is.na(train[,paste("failure", sep = "_", i + 1)])),]
        
        # Get predictors matrix (with no "inv" and "interest_diff" for 2008)
        X <- as.data.frame(cbind(data_model[paste(lagged_predictors, sep = "_", i)], 
                                 data_model[grep("nace", colnames(data_model))], 
                                 data_model[grep("area", colnames(data_model))]))
        
        # XGBoost requires matrix format for the predictor matrix
        X <- matrix(unlist(X), ncol = length(X), nrow = nrow(X))
        
        # Get label vector
        y <- as.vector(data_model[,paste("failure", sep = "_", i + 1)])
        
        # Run XGboost
        set.seed(2020)
        xgboost <- xgboost(data = X, label = y, objective = "binary:logistic", 
                           nrounds = 100, 
                           verbose = 0)
        
        # Get fitted values
        prob[which(!is.na(train[,paste("failure", sep = "_", i + 1)])), which(years==i)] <-  predict(xgboost, newdata = X, type = "response" ) 
    
    
    } else {
      
        # Subset data by year
        data_model <- train[which(!is.na(train[,paste("failure", sep = "_", i + 1)])),]
        
        # Get predictors matrix (from 2009 onwards we now have "inv" and "interest_diff") and the one-hot encoded predictors
        X <- as.data.frame(cbind(data_model[,paste(lagged_predictors, sep = "_", i)], 
                                 data_model[,paste(outside, sep = "_", i)],
                                 data_model[grep("nace", colnames(data_model))], 
                                 data_model[grep("area", colnames(data_model))]))
        
        # XGBoost requires matrix format for the predictor matrix
        X <- matrix(unlist(X), ncol = length(X), nrow = nrow(X))
        
        # Get label vector
        y <- as.vector(data_model[,paste("failure", sep = "_", i + 1)])
        
        # Run XGBoost
        set.seed(2020)
        xgboost <- xgboost(data = X, label = y, objective = "binary:logistic", nrounds = 100, verbose = 0)
        
        # Get fitted values
        prob[which(!is.na(train[,paste("failure", sep = "_", i + 1)])), which(years==i)] <-  predict(xgboost, newdata = X, type = "response" )   
      
      
    }
}

# Save Results
train$prob_2009 <- prob[,1]
train$prob_2010 <- prob[,2]
train$prob_2011 <- prob[,3]
train$prob_2012 <- prob[,4]
train$prob_2013 <- prob[,5]
train$prob_2014 <- prob[,6]
train$prob_2015 <- prob[,7]
train$prob_2016 <- prob[,8]
train$prob_2017 <- prob[,9]
```

Save data augmented of default probabilities
```{r}
# Rename one-hot encoded variables because "." is considered an illegal character
colnames(train) = gsub("nace.", "nace_", colnames(train))
colnames(train) = gsub("area.", "area_", colnames(train))

# Save new data

# Uncomment if liquidations have been dropped 
# write_dta(train, "C:/Users/fabio/OneDrive/Documenti/GitHub/machine-learning-for-zombie-hunting/R_code/REVIEW_year_by_year_analyses/data_year_by_year.dta")

# Liquidations included
write_dta(train, "C:/Users/fabio/OneDrive/Documenti/GitHub/machine-learning-for-zombie-hunting/R_code/year_by_year_analyses/data_liq_year_by_year.dta")
```

Load previously saved data
```{r}
train <- read_dta("C:/Users/fabio/OneDrive/Documenti/GitHub/machine-learning-for-zombie-hunting/R_code/year_by_year_analyses/data_liq_year_by_year.dta")
```


# Statistics: in-sample performance over years
```{r eval=FALSE}

years <- c(2014:2017)
for (i in (years)) {

index_failure <- paste("failure", i, sep = "_")
index_prob <- paste("prob", i, sep = "_")


data_model <- train[which(!is.na(train[ , eval(index_failure)])), ]
data_model[[eval(index_failure)]] <- as.factor(data_model[[eval(index_failure)]])

# Separating the predictions of firms who actually failed in year i from those who don't
fg.xgboost<-data_model[[eval(index_prob)]][ data_model[[eval(index_failure)]]==1 ] 
bg.xgboost<-data_model[[eval(index_prob)]][ data_model[[eval(index_failure)]]==0 ] 


# ROC curve
roc_xgboost<-roc.curve(scores.class0 = fg.xgboost,
                    scores.class1 = bg.xgboost,
                    curve = T)
#jpeg(file="roc_xgboost.jpg")
#plot(roc_xgboost)
#dev.off()


# Area under the PR curve
pr_xgboost<-pr.curve(scores.class0 = fg.xgboost,
                  scores.class1 = bg.xgboost,
                  curve = T)
# jpeg(file="pr_xgboost.jpg")
# plot(pr_xgboost)
# dev.off()


# Approccio data-driven: predico che fallisci se stai sopra alla mediana o ad un certo percentile
# Approccio "teorico": fisso una soglia arbitraria (es. 0.03)
#  
fitted.xgboost <- ifelse(data_model[[eval(index_prob)]]> quantile(data_model[[eval(index_prob)]], 
                                                               probs = c(0.5), na.rm = TRUE), 1, 0)
fitted.xgboost <- as.factor(fitted.xgboost)

# F1-Score and BACC
conf_matrix <- confusionMatrix(as.factor(fitted.xgboost), as.factor(data_model[[eval(index_failure)]]), mode = "everything", positive="1") 

f1_xgboost <- conf_matrix[[4]][["F1"]]
balanced_accuracy_xgboost <- conf_matrix[[4]][["Balanced Accuracy"]]

# Rsquared 
accuracy_xgboost <- as.data.frame(rbind(postResample(as.double(fitted.xgboost), data_model[[eval(index_prob)]])))


# Append all statistics
if(which(years==i)==1) {
     xgboost_fit <- as.data.frame(cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost,
                                      balanced_accuracy_xgboost, accuracy_xgboost$Rsquared))
     
}
else{
     xgboost_fit <- rbind(xgboost_fit, cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost, 
                            balanced_accuracy_xgboost, accuracy_xgboost$Rsquared  ) )
      
}
}
colnames(xgboost_fit) <- c("Year", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared")
xgboost_fit
```








# Out-of-range values imputation: check yearly performances

```{r eval=FALSE}
# Get one-hot encoded data.frame
dummy = dummyVars(" ~ nace + area", data = data_italy)
temp = data.frame(predict(dummy, newdata = data_italy))

# Add one-hot encoded variables to the original data
train = cbind(data_italy, temp)

# Drop the original nace and area variables
train = within(train, rm("nace", "area"))


labels = c("failure_2009", "failure_2010", "failure_2011", "failure_2012", "failure_2013",
           "failure_2014", "failure_2015", "failure_2016", "failure_2017")

temp = train[!names(train) %in% labels]
temp[is.na(temp)] = 10^20
new_train = cbind(train[labels], temp)

```

```{r eval=FALSE}
years <- c(2008:2016)
prob <- matrix(NA, ncol = length(years), nrow = nrow(new_train))

for (i in (years)){
    
    if (i == 2008) {

        # Subset data by year
        data_model <- new_train[which(!is.na(new_train[,paste("failure", sep = "_", i + 1)])),]
        
        # Get predictors matrix (with no "inv" and "interest_diff" for 2008)
        X <- as.data.frame(cbind(data_model[paste(lagged_predictors, sep = "_", i)], 
                                 data_model[grep("nace", colnames(data_model))], 
                                 data_model[grep("area", colnames(data_model))]))
        
        # XGBoost requires matrix format for the predictor matrix
        X <- matrix(unlist(X), ncol = length(X), nrow = nrow(X))
        
        # Get label vector
        y <- as.vector(data_model[,paste("failure", sep = "_", i + 1)])
        
        # Run XGboost
        set.seed(2020)
        xgboost <- xgboost(data = X, label = y, objective = "binary:logistic", nrounds = 100, verbose = 0)
        
        # Get fitted values
        prob[which(!is.na(new_train[,paste("failure", sep = "_", i + 1)])), which(years==i)] <-  predict(xgboost, newdata = X, type = "response" ) 
    
    
    } else {
      
        # Subset data by year
        data_model <- new_train[which(!is.na(new_train[,paste("failure", sep = "_", i + 1)])),]
        
        # Get predictors matrix (from 2009 onwards we now have "inv" and "interest_diff") and the one-hot encoded predictors
        X <- as.data.frame(cbind(data_model[,paste(lagged_predictors, sep = "_", i)], 
                                 data_model[,paste(outside, sep = "_", i)],
                                 data_model[grep("nace", colnames(data_model))], 
                                 data_model[grep("area", colnames(data_model))]))
        
        # XGBoost requires matrix format for the predictor matrix
        X <- matrix(unlist(X), ncol = length(X), nrow = nrow(X))
        
        # Get label vector
        y <- as.vector(data_model[,paste("failure", sep = "_", i + 1)])
        
        # Run XGBoost
        set.seed(2020)
        xgboost <- xgboost(data = X, label = y, objective = "binary:logistic", nrounds = 100, verbose = 0)
        
        # Get fitted values
        prob[which(!is.na(new_train[,paste("failure", sep = "_", i + 1)])), which(years==i)] <-  predict(xgboost, newdata = X, type = "response" )   
      
      
    }
}

# Save Results
new_train$prob_2009 <- prob[,1]
new_train$prob_2010 <- prob[,2]
new_train$prob_2011 <- prob[,3]
new_train$prob_2012 <- prob[,4]
new_train$prob_2013 <- prob[,5]
new_train$prob_2014 <- prob[,6]
new_train$prob_2015 <- prob[,7]
new_train$prob_2016 <- prob[,8]
new_train$prob_2017 <- prob[,9]
```


Statistics
```{r eval=FALSE}
years <- c(2014:2017)
for (i in (years)) {

index_failure <- paste("failure", i, sep = "_")
index_prob <- paste("prob", i, sep = "_")


data_model <- new_train[which(!is.na(new_train[ , eval(index_failure)])), ]
data_model[[eval(index_failure)]] <- as.factor(data_model[[eval(index_failure)]])

# Separating the predictions of firms who actually failed in year i from those who don't
fg.xgboost<-data_model[[eval(index_prob)]][ data_model[[eval(index_failure)]]==1 ] 
bg.xgboost<-data_model[[eval(index_prob)]][ data_model[[eval(index_failure)]]==0 ] 


# ROC curve
roc_xgboost<-roc.curve(scores.class0 = fg.xgboost,
                    scores.class1 = bg.xgboost,
                    curve = T)
#jpeg(file="roc_xgboost.jpg")
#plot(roc_xgboost)
#dev.off()


# Area under the PR curve
pr_xgboost<-pr.curve(scores.class0 = fg.xgboost,
                  scores.class1 = bg.xgboost,
                  curve = T)
# jpeg(file="pr_xgboost.jpg")
# plot(pr_xgboost)
# dev.off()


# Approccio data-driven: predico che fallisci se stai sopra alla mediana o ad un certo percentile
# Approccio "teorico": fisso una soglia arbitraria (es. 0.03)
#  
fitted.xgboost <- ifelse(data_model[[eval(index_prob)]]> quantile(data_model[[eval(index_prob)]], 
                                                               probs = c(0.5), na.rm = TRUE), 1, 0)
fitted.xgboost <- as.factor(fitted.xgboost)

# F1-Score and BACC
conf_matrix <- confusionMatrix(as.factor(fitted.xgboost), as.factor(data_model[[eval(index_failure)]]), mode = "everything", positive="1") 

f1_xgboost <- conf_matrix[[4]][["F1"]]
balanced_accuracy_xgboost <- conf_matrix[[4]][["Balanced Accuracy"]]

# Rsquared 
accuracy_xgboost <- as.data.frame(rbind(postResample(as.double(fitted.xgboost), data_model[[eval(index_prob)]])))


# Append all statistics
if(which(years==i)==1) {
     xgboost_fit <- as.data.frame(cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost,
                                      balanced_accuracy_xgboost, accuracy_xgboost$Rsquared))
     
}
else{
     xgboost_fit <- rbind(xgboost_fit, cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost, 
                            balanced_accuracy_xgboost, accuracy_xgboost$Rsquared  ) )
      
}
}
colnames(xgboost_fit) <- c("Year", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared")
xgboost_fit
```















# Figure 9: compute the share of zombie firms over time

We consider a firm as a zombie if lying above the 9th decile for three consecutive years without failing (i.e. False Positive)

Fix the quantile threshold discriminating exit and non-exit prediction (where we expect lie most zombie firms)
```{r}
cutoff = 0.90
```


Get a vectorized confusion matrix in each year by comparing:
- the threshold-based (integer) fitted values
- the actual failure indicator (label)
i.e., each non-missing observation in the fitted value is assigned a category: TP, FP, TN, FN
```{r}
years <- c(2009:2017)
train <- data.frame(train) 
categories_confusion_matrix <- data.frame(matrix(NA, ncol = length(years), nrow = nrow(train)))

for (i in years) {
  # Get non-missing observation in prob_i (and consequently in failure_i)
  index = which(!is.na(train[paste("prob", sep="_", i)]))
  
  # Transform numeric predictions into integer according to the cutoff
  fitted.complete = train[index, paste("prob", sep="_", i)]
  fitted.integer = ifelse(fitted.complete > quantile(fitted.complete, probs = cutoff, na.rm =TRUE), 1, 0)
  
  # Convert predicted and actual values into factors
  fitted.integer = as.factor(fitted.integer)
  ground_truth = as.factor(train[index, paste("failure", sep="_", i)]) 
  
  # Assign to each firm its confusion matrix category
  vectorized_conf_matrix = matrix(NA, ncol = 1, nrow = length(fitted.integer))
  vectorized_conf_matrix[which(fitted.integer == 1 & ground_truth == 0)] = "FP"    # False positives: potential ZOMBIES if resulting FP for 3 consecutive years 
  vectorized_conf_matrix[which(fitted.integer == 1 & ground_truth == 1)] = "TP"    # True positives: failed firms above the 9th decile
  vectorized_conf_matrix[which(fitted.integer == 0 & ground_truth == 1)] = "FN"    # False negatives: failed firms below the 9th decile
  vectorized_conf_matrix[which(fitted.integer == 0 & ground_truth == 0)] = "TN"    # True negatives: non-failing firms below the 9th decile
  
  # Transform into factor
  vectorized_conf_matrix = factor(vectorized_conf_matrix, levels = c("FP", "TP", "FN", "TN"))
  
  # Levels will now be encoded in categories_confusion_matrix as: 1 (FP), 2 (TP), 3 (FN), 4 (TN)
  categories_confusion_matrix[index, which(years ==i)] = vectorized_conf_matrix
}

# Rename columns
names = c()
for (i in years) {
  names = append(names, paste("cat_conf_matrix", sep = "_", i))
  
}
colnames(categories_confusion_matrix) = names
```


Identify zombie firms: FP for 3 consecutive years (we get zombies for years 2011-2017)
```{r}
years <- c(2011:2017)

zombie <- data.frame(matrix(NA, ncol = length(years), nrow = nrow(categories_confusion_matrix)))

# Identifying ZOMBIES: firms resulting in the 9th decile for 3 consecutive years (put NA if the condition is not met)
for (i in years) {
  zombie[which(years ==i)] <- ifelse(categories_confusion_matrix[ , paste("cat_conf_matrix", sep = "_", c(i-2))] == 1 &
                                     categories_confusion_matrix[ , paste("cat_conf_matrix", sep = "_", c(i-1))] == 1 & 
                                     categories_confusion_matrix[ , paste("cat_conf_matrix", sep = "_", c(i))] == 1, 1, NA)
}

# Identifying NO ZOMBIES: firms not lying in the 9th decile AND non-missing in the decile category for 3 consecutive years 
# 
# NB: These are companies that we know for sure do not meet the "zombieness" conditions, as they have, for example, (1,1,4), (2,3,4), or other combinations, but they are not missing in any of the 3 decile categories involved in the zombieness of a certain year.


for (i in years) {
  zombie_index = which(zombie[, which(years ==i)] == 1)
  
  no_missing_index = which(!is.na(categories_confusion_matrix[ , paste("cat_conf_matrix", sep = "_", c(i-2))]) & 
                           !is.na(categories_confusion_matrix[ , paste("cat_conf_matrix", sep = "_", c(i-1))]) & 
                           !is.na(categories_confusion_matrix[ , paste("cat_conf_matrix", sep = "_", c(i))  ]) )
  
  overall_index = no_missing_index[!(no_missing_index %in% zombie_index)]
  
  
  zombie[overall_index, which(years ==i)] <- 0
}



# Rename columns
names = c()
for (i in years) {
  names = append(names, paste("zombie", sep = "_", i))
  
}
colnames(zombie) = names
```


Number and share of zombie firms overtime
```{r}
print("Number of zombie firms")
sapply(zombie, function(x) sum(x == 1, na.rm = TRUE))

# We don't use the command sapply(zombie, percent.table) because percent.table returns percentages that don't add up to 1, as the remaining part consists of missing values. 
# Therefore, it underestimates the share of zombies compared to firms for which we observe the zombie/non-zombie status in a certain year.

print("Share of zombie firms")
sapply(zombie, function(x) length(which(x ==1)) / length(which(!is.na(x)))   ) * 100

print("Non-missing")
sapply(zombie, function(x) length(which(!is.na(x)))  )

prova <- sapply(zombie, function(x) length(which(x ==1)) / length(which(!is.na(x)))   ) * 100 
```


Plot the share of zombie firms (FP for three consectuvie years based on 0.9 cutoff) against GDP 
```{r eval=FALSE}
zombie_share <- prova
gdp_growth <- c(0.707, -2.981, -1.841, -0.005, 0.778, 1.280, 1.716) # Data from World Bank (https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?end=2018&locations=IT&start=2011)
unemployment <- c(8.4, 10.7, 12.1, 12.7, 11.9, 11.7, 11.2) # Data from Statista (https://www.statista.com/statistics/531010/unemployment-rate-italy/)
years <- c(seq(2011,2017,1))


twoord.plot(lx = years, ly = zombie_share,
            rx = years, ry = gdp_growth,
            type= c("l", "l"),
            lwd = 3,
            #yaxt = 'n',
            lylim = range(zombie_share) + c(-0.2,0.2),
            rylim = range(gdp_growth) + c(-0.5,0.5),
            lcol = "blue",
            xlab = "Years",
            ylab = "Zombie rate",
            rylab = "GDP growth rate",
            #main = "Share of Zombie Firms and GDP growth",
            do.first="plot_bg();grid(col=\"white\",lty=1)")

# twoord.plot(lx = years, ly = zombie_share,
#             rx = years, ry = unemployment,
#             type= c("l", "l"),
#             lwd = 3, 
#             yaxt = 'n',
#             lylim = range(zombie_share) + c(-0.5,0.5),
#             rylim = range(unemployment) + c(-0.5,0.5),
#             lcol = 4,
#             rcol="coral4",
#             xlab = "Years",
#             ylab = "Zombie rate",
#             rylab = "Unemployment rate rate",
#             #main = "Share of Zombie Firms and Unemployment Rate",
#             do.first="plot_bg();grid(col=\"white\",lty=1)")
```


# Testing other pupular criteria for zombieness and compare with XGBoost

## ICR (McGowan et al., 2018; Banerjee & Hoffman, 2018)

### Geography (NUTS 2)

Identify firms ICR < 1 for 3 consecutive years (we get zombies for years 2011-2016 since our last available ICR is on 2016)
```{r}
years <- c(2011:2016)

zombie_icr <- data.frame(matrix(NA, ncol = length(years), nrow = nrow(train)))

# Identifying ICR-based zombies: (ICR< 1 for 3 consecutive years, age >= 10, not failed). Put NA if this condition is not met. 
for (i in years) {
  zombie_icr[ , which(years ==i)] <- ifelse(train[ , paste("ICR_failure", sep = "_", c(i-2))] == 1 &
                                         train[ , paste("ICR_failure", sep = "_", c(i-1))] == 1 & 
                                         train[ , paste("ICR_failure", sep = "_", c(i))] == 1 &
                                         train[ , paste("failure", sep = "_", c(i))] == 0 &   
                                         train[ , "age"] >= 10, 1, NA)
}

# Identifying NO ICR-based zombies: firms no satisfying the condition: (ICR< 1 for 3 consecutive years, age >= 10, not failed)
# (sono imprese che so per certo non soddisfino questa condizione, ma non sono missing nell'ICR per 3 anni, in age e in failure
for (i in years) {
  zombie_icr_index = which(zombie_icr[, which(years ==i)] == 1)
  
  no_missing_index = which(!is.na(train[ , paste("ICR_failure", sep = "_", c(i-2))]) & 
                           !is.na(train[ , paste("ICR_failure", sep = "_", c(i-1))]) & 
                           !is.na(train[ , paste("ICR_failure", sep = "_", c(i))  ]) &
                            (train[ , paste("failure", sep = "_", c(i))] == 0 | train[ , paste("failure", sep = "_", c(i))] == 1) & 
                           !is.na(train[ , "age"]))
  
  overall_index = no_missing_index[!(no_missing_index %in% zombie_icr_index)]
  
  
  zombie_icr[overall_index, which(years ==i)] <- 0
}



# Rename columns
names = c()
for (i in years) {
  names = append(names, paste("zombie_icr", sep = "_", i))
  
}
colnames(zombie_icr) = names
```



```{r}
print("Number of firms with critical ICR")
sapply(zombie_icr, function(x) sum(x == 1, na.rm = TRUE))

print("Share of of firms with critical ICR")
sapply(zombie_icr, function(x) length(which(x ==1)) / length(which(!is.na(x)))   ) * 100

print("Non-missing ICR")
sapply(zombie_icr, function(x) length(which(!is.na(x)))  )
```

Get firms that are for at least one year either zombies or with critical ICR
```{r}
zombie_icr_1_year = which(rowSums(zombie_icr, na.rm = TRUE) > 0)
zombie_xgboost_1_year = which(rowSums(zombie, na.rm = TRUE) > 0)

# Zombies detected by both criteria (XGBoost and ICR)
intersection = zombie_xgboost_1_year[zombie_xgboost_1_year %in% zombie_icr_1_year]

# XGBoost zombies not detected by ICR critetion
XGB_exclusive_zombie = zombie_xgboost_1_year[!(zombie_xgboost_1_year %in% zombie_icr_1_year)]

# ICR zombies not detected by XGBoost
ICR_exclusive_zombie = zombie_icr_1_year[!(zombie_icr_1_year %in% zombie_xgboost_1_year)]
```

Create categorical variable:

- 1 if you have been detected from both XGBoost and ICR (common support)

- 2 only XGBoost

- 3 only ICR
```{r}
zombie_compare = data.frame(matrix(NA, ncol = 1, nrow = nrow(train)))
colnames(zombie_compare) = c("zombie_compare")

# Create categorical variable: 1 = common support, 2 = exclusive XGBoost zombies, 3 = esclusive ICR zombies
zombie_compare[intersection, ] = 1
zombie_compare[XGB_exclusive_zombie, ] = 2
zombie_compare[ICR_exclusive_zombie, ] = 3

# Merge the categorical variable to the original data
data_zombie_xgb_ICR  = cbind(train, zombie_compare)

# Save firms catched overall by the two criteria
zombies_xgboost_icr = data_zombie_xgb_ICR[c(intersection, XGB_exclusive_zombie, ICR_exclusive_zombie), ]
```


Preparare data for radarchart (see https://r-graph-gallery.com/143-spider-chart-with-saveral-individuals.html)
```{r}
# Select data of interest for radar plot
data_plot = data_zombie_xgb_ICR[ , c("region", "zombie_compare")]

# Keep only zombies (either indentified by both algorithm or at least one)
data_plot = data_plot[complete.cases(data_plot), ]

# Convert variables into factor
data_plot$region = as.factor(data_plot$region)
data_plot$zombie_compare = as.factor(data_plot$zombie_compare)

# Create contingency table: how many zombies fall in each region, for each category (common support / only XGBoost / only ICR)
table = table(data_plot$zombie_compare, data_plot$region)

# Convert into frequency by region 
data_plot = data.frame(prop.table(table, m=2))
data_plot = data_plot[4:nrow(data_plot), ]

# Reshape data for radarchart requirements
data_plot = reshape(data_plot, varying = levels(data_plot$Var2)[2:21], idvar = "Var1", timevar = "Var2", direction = "wide")

# Remove the first column (useless)
data_plot = data_plot[ , 2:ncol(data_plot)] 

# Rename rows 
row.names(data_plot) = c("Intersection", "Only XGBoost", "Only ICR")

# Add upper and lower bounds of the radar by adding two lines
data_plot <- rbind(rep(1, ncol(data_plot)) , rep(0,ncol(data_plot)) , data_plot)
data_plot <- round(data_plot, digits = 2)

# Rename regions
colnames(data_plot) = c("ITF1", "ITF5", "ITF6", "ITF3", "ITH5", "ITH4","ITI4","ITC3","ITC4","ITI3","ITF2","ITC1","ITF4","ITG2","ITG1", "ITH1/2", "ITI1", "ITI2", "ITC2", "ITH3")
#colnames(data_plot) = c("ABR", "BAS", "CAL", "CAM", "EMR", "FVG", "LAZ", "LIG", "LOM", "MAR", "MOL", "PIE", "PUG", "SAR", "SIC", "TAA", "TOS", "UMB", "VDA", "VEN")

# Set up colors 
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )
```

Plot radarchart comparison of zombies among regions
```{r}
radarchart(data_plot, 
           axistype = 1, 
           #custom polygon
           pcol=colors_border , pfcol=colors_in , plwd = 1 , plty = 1, pty = c(15,16,17),
           #custom the grid
           cglcol="grey", cglty=1, axislabcol="grey", 
           caxislabels = seq(from = 0, to = 1, by = 0.25),
           cglwd=0.8,
           #custom labels
           vlcex=0.8)

legend(x = 1, y = 1.4, legend = c("Common support", "MA-XGBoost", "ICR"), bty = "n", pch = c(15,16,17) , col=colors_in , text.col = "black", cex=0.7, pt.cex=1.5)

# colnames(data_plot)[16] <- "ITH1_2"
# write_dta(data_plot[3:5, ], "C:/Users/fabio/OneDrive/Desktop/zombie_nace_ICR.dta")
```


Save radar chart in table format 
```{r eval = FALSE}
data_plot = t(data_plot)
write.csv(data_plot[,3:5], "C:/Users/fabio/OneDrive/Desktop/zombie_nuts_ICR.csv")
```

### Industries

Upload original data (in "train" we do not have NACE since it has been converted in dummy with one-hot encoding for XGBoost). We need original italian data with NACE 
```{r}
data <- read_dta("analysis_data_indicators.dta")

# Get Italian data
data_italy <- data[which(data$iso=="IT"),] 

names(data_italy)[names(data_italy) == 'GUO___BvD_ID_number'] <- 'guo'

data_italy$control <- ifelse(data_italy$guo=="", 0, 1)
data_italy$nace <- as.factor(data_italy$nace)
data_italy$area <- as.factor(data_italy$area)

# Convert our 4-digits NACE into 2 digits
levels(data_italy$nace) <- floor(as.numeric(levels(data_italy$nace))/100) 
```


Preparare data for radar chart plot by industries (NACE 2-digits)
```{r}
zombie_compare = data.frame(matrix(NA, ncol = 1, nrow = nrow(train)))
colnames(zombie_compare) = c("zombie_compare")

# Create categorical variable: 1 = common support, 2 = exclusive XGBoost zombies, 3 = esclusive ICR zombies
zombie_compare[intersection, ] = 1
zombie_compare[XGB_exclusive_zombie, ] = 2
zombie_compare[ICR_exclusive_zombie, ] = 3

# Merge the categorical variable to the original data
NACE_data_zombie_xgb_ICR  = cbind(data_italy, zombie_compare)

# Save firms catched overall by the two criteria
NACE_zombies_xgboost_icr = NACE_data_zombie_xgb_ICR[c(intersection, XGB_exclusive_zombie, ICR_exclusive_zombie), ]

# Select data of interest for radar plot
data_plot = NACE_zombies_xgboost_icr[, c("nace", "zombie_compare")]

# Keep only zombies (either indentified by both algorithm or at least one)
data_plot = data_plot[complete.cases(data_plot), ]

# Convert variables into factor
data_plot$region = as.factor(data_plot$nace)
data_plot$zombie_compare = as.factor(data_plot$zombie_compare)

# Create contingency table: how many zombies fall in each region, for each category (common support / only XGBoost / only ICR)
table = table(data_plot$zombie_compare, data_plot$nace)

# Convert into frequency by region 
data_plot = data.frame(prop.table(table, m=2))
data_plot = data_plot[4:nrow(data_plot), ]

# Reshape data for radarchart requirements
data_plot = reshape(data_plot, varying = levels(data_plot$Var2), idvar = "Var1", timevar = "Var2", direction = "wide")

# Remove the first column (useless)
data_plot = data_plot[ , 2:ncol(data_plot)] 

# Rename rows
row.names(data_plot) = c("Intersection", "Only XGBoost", "Only ICR")

# Add upper and lower bounds of the radar by adding two lines
data_plot <- rbind(rep(1, ncol(data_plot)) , rep(0,ncol(data_plot)) , data_plot)
data_plot <- round(data_plot, digits = 2)


# Set up colors
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )
```

Plot radar chart
```{r}
radarchart(data_plot, 
           axistype = 1, 
           #custom polygon
           pcol=colors_border , pfcol=colors_in , plwd = 1 , plty = 1, pty = c(15,16,17),
           #custom the grid
           cglcol="grey", cglty=1, axislabcol="grey", 
           caxislabels = seq(from = 0, to = 1, by = 0.25),
           cglwd=0.8,
           #custom labels
           vlcex=0.8)

legend(x = 1, y = 1.4, legend = c("Common support", "MA-XGBoost", "ICR"), bty = "n", pch = c(15,16,17) , col=colors_in , text.col = "black", cex=0.7, pt.cex=1.5)
```





## Negative value added (Bank of Korea, 2013)

### Geography (NUTS 2) 

Identify firms with negative value added in a year, without failure (we get them for years 2011-2016 since our last available value added is on 2016)
```{r}
years <- c(2011:2016)

zombie_NEG_VA <- data.frame(matrix(NA, ncol = length(years), nrow = nrow(train)))

# Identifying negative added value-based firms: those with negative value added who did not fail (put NA if the condition is not met) 
for (i in years) {
  zombie_NEG_VA[ , which(years ==i)] <- ifelse(train[ , paste("NEG_VA", sep = "_", c(i))] == 1 &
                                               train[ , paste("failure", sep = "_", c(i))] == 0, 1, NA)
  
}

# Identifying NO critical value added firms: firms no satisfying the condition: (NEG_VA = 1 & not failed) 
# These are companies that we are certain do not meet the crucial negative value added condition, even though they are not missing in NEG_VA and failure.
for (i in years) {
  zombie_NEG_VA_index = which(zombie_NEG_VA[, which(years ==i)] == 1)
  
  no_missing_index = which(!is.na(train[ , paste("NEG_VA", sep = "_", c(i))  ]) &
                            (train[ , paste("failure", sep = "_", c(i))] == 0 | train[ , paste("failure", sep = "_", c(i))] == 1) )
  
  
  overall_index = no_missing_index[!(no_missing_index %in% zombie_NEG_VA_index)]
  
  
  zombie_NEG_VA[overall_index, which(years ==i)] <- 0
}



# Rename columns
names = c()
for (i in years) {
  names = append(names, paste("zombie_NEG_VA", sep = "_", i))
  
}
colnames(zombie_NEG_VA) = names
```



```{r}
print("Number of NEG_VA zombie firms")
sapply(zombie_NEG_VA, function(x) sum(x == 1, na.rm = TRUE))

# non uso sapply(zombie, percent.table) perché percent.table mi da delle % che non sommano a 1 perché la parte rimanente sono missing values: sottostima pertanto la quota di imprese zombie rispetto alle imprese di cui osserviamo lo status zombie/non-zombie in un certo anno

print("Share of NEG_VA zombie firms")
sapply(zombie_NEG_VA, function(x) length(which(x ==1)) / length(which(!is.na(x)))   ) * 100

print("Non-missing NEG_VA")
sapply(zombie_NEG_VA, function(x) length(which(!is.na(x)))  )
```

Get firms that are for at least one year either zombies or with negative value added
```{r}
zombie_NEG_VA_1_year = which(rowSums(zombie_NEG_VA, na.rm = TRUE) > 0)
zombie_xgboost_1_year = which(rowSums(zombie, na.rm = TRUE) > 0)

# Zombies detected by both criteria (XGBoost and NEG_VA)
intersection = zombie_xgboost_1_year[zombie_xgboost_1_year %in% zombie_NEG_VA_1_year]

# XGBoost zombies not detected by NEG_VA criterion
XGB_exclusive_zombie = zombie_xgboost_1_year[!(zombie_xgboost_1_year %in% zombie_NEG_VA_1_year)]

# NEG_VA zombies not detected by XGBoost
NEG_VA_exclusive_zombie = zombie_NEG_VA_1_year[!(zombie_NEG_VA_1_year %in% zombie_xgboost_1_year)]
```

Create categorical variable:

- 1 if you have been detected from both XGBoost and NEG_VA

- 2 only XGBoost

- 3 only negative value-added
```{r}
zombie_compare = data.frame(matrix(NA, ncol = 1, nrow = nrow(train)))
colnames(zombie_compare) = c("zombie_compare")

# Create categorical variable: 1 = common support, 2 = exclusive XGBoost zombies, 3 = esclusive NEG_VA zombies
zombie_compare[intersection, ] = 1
zombie_compare[XGB_exclusive_zombie, ] = 2
zombie_compare[NEG_VA_exclusive_zombie, ] = 3

# Merge the categorical variable to the original data
data_zombie_xgb_NEG_VA = cbind(train, zombie_compare)

# Get zombie data
zombies_xgboost_NEG_VA = train[c(intersection, XGB_exclusive_zombie, NEG_VA_exclusive_zombie), ]
```


Preparare data for radarchart (see https://r-graph-gallery.com/143-spider-chart-with-saveral-individuals.html)
```{r}
# Select data of interest for radar plot
data_plot = data_zombie_xgb_NEG_VA[ , c("region", "zombie_compare")]

# Keep only zombies (either indentified by both algorithm or at least one)
data_plot = data_plot[complete.cases(data_plot), ]

# Convert variables into factor
data_plot$region = as.factor(data_plot$region)
data_plot$zombie_compare = as.factor(data_plot$zombie_compare)

# Create contingency table: how many zombies fall in each region, for each category (common support / only XGBoost / only ICR)
table = table(data_plot$zombie_compare, data_plot$region)

# Convert into frequency by region 
data_plot = data.frame(prop.table(table, m=2))

# Reshape data for radarchart requirements
data_plot = reshape(data_plot, varying = levels(data_plot$Var2), idvar = "Var1", timevar = "Var2", direction = "wide")

# Remove the first column (useless)
data_plot = data_plot[ , 2:ncol(data_plot)] 

# Rename rows 
row.names(data_plot) = c("Intersection", "Only XGBoost", "Only NEG_VA")

# Add upper and lower bounds of the radar by adding two lines
data_plot <- rbind(rep(1, ncol(data_plot)) , rep(0,ncol(data_plot)) , data_plot)
data_plot <- round(data_plot, digits = 2)

# Rename regions
colnames(data_plot) = c("ITF1", "ITF5", "ITF6", "ITF3", "ITH5", "ITH4","ITI4","ITC3","ITC4","ITI3","ITF2","ITC1","ITF4","ITG2","ITG1", "ITH1/2", "ITI1", "ITI2", "ITC2", "ITH3")
#colnames(data_plot) = c("ABR", "BAS", "CAL", "CAM", "EMR", "FVG", "LAZ", "LIG", "LOM", "MAR", "MOL", "PIE", "PUG", "SAR", "SIC", "TAA", "TOS", "UMB", "VDA", "VEN")

# Set up colors 
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )
```

Plot radarchart comparison of zombies among regions
```{r}
radarchart(data_plot, 
           axistype = 1, 
           #custom polygon
           pcol=colors_border , pfcol=colors_in , plwd = 1 , plty = 1, pty = c(15,16,17),
           #custom the grid
           cglcol="grey", cglty=1, axislabcol="grey", 
           caxislabels = seq(from = 0, to = 1, by = 0.25),
           cglwd=0.8,
           #custom labels
           vlcex=0.8)

legend(x = 1, y = 1.4, legend = c("Common support", "MA-XGBoost", "Negative value added"), bty = "n", pch = c(15,16,17) , col=colors_in , text.col = "black", cex=0.7, pt.cex=1.3)
```

```{r}
colnames(data_plot)[16] <- "ITH1_2"
write_dta(data_plot[3:5, ], "C:/Users/fabio/OneDrive/Desktop/zombie_nace_NEG_VA.dta")
```



### Industries 
```{r}
zombie_compare = data.frame(matrix(NA, ncol = 1, nrow = nrow(train)))
colnames(zombie_compare) = c("zombie_compare")

# Create categorical variable: 1 = common support, 2 = exclusive XGBoost zombies, 3 = esclusive ICR zombies
zombie_compare[intersection, ] = 1
zombie_compare[XGB_exclusive_zombie, ] = 2
zombie_compare[NEG_VA_exclusive_zombie, ] = 3

# Merge the categorical variable to the original data
NACE_data_zombie_xgb_NEG_VA  = cbind(data_italy, zombie_compare)

# Save zombies catched overall by the two criteria
NACE_zombies_xgboost_NEG_VA = NACE_data_zombie_xgb_NEG_VA[c(intersection, XGB_exclusive_zombie, NEG_VA_exclusive_zombie), ]


# Select data of interest for radar plot
data_plot = NACE_zombies_xgboost_NEG_VA[, c("nace", "zombie_compare")]

# Keep only zombies (either indentified by both algorithm or at least one)
data_plot = data_plot[complete.cases(data_plot), ]

# Convert variables into factor
data_plot$region = as.factor(data_plot$nace)
data_plot$zombie_compare = as.factor(data_plot$zombie_compare)

# Create contingency table: how many zombies fall in each region, for each category (common support / only XGBoost / only ICR)
table = table(data_plot$zombie_compare, data_plot$nace)

# Convert into frequency by region 
data_plot = data.frame(prop.table(table, m=2))
data_plot = data_plot[4:nrow(data_plot), ]

# Reshape data for radarchart requirements
data_plot = reshape(data_plot, varying = levels(data_plot$Var2), idvar = "Var1", timevar = "Var2", direction = "wide")

# Remove the first column (useless)
data_plot = data_plot[ , 2:ncol(data_plot)] 

# Rename rows
row.names(data_plot) = c("Intersection", "Only XGBoost", "Only NEG_VA")

# Add upper and lower bounds of the radar by adding two lines
data_plot <- rbind(rep(1, ncol(data_plot)) , rep(0,ncol(data_plot)) , data_plot)
data_plot <- round(data_plot, digits = 2)

# Rename regions
#colnames(data_plot) = c("Food products", "Beverages", "Tobacco products", "Textiles", "Wearing apparel", "Leather and related products","Wood ecc.","Paper products","","ITI3","ITF2","ITC1","ITF4","ITG2","ITG1", "ITH1/2", "ITI1", "ITI2", "ITC2", "ITH3")


# Set up colors
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )
```


```{r}
radarchart(data_plot, 
           axistype = 1, 
           #custom polygon
           pcol=colors_border , pfcol=colors_in , plwd = 1 , plty = 1, pty = c(15,16,17),
           #custom the grid
           cglcol="grey", cglty=1, axislabcol="grey", 
           caxislabels = seq(from = 0, to = 1, by = 0.25),
           cglwd=0.8,
           #custom labels
           vlcex=0.8)

legend(x = 1, y = 1.4, legend = c("Common support", "MA-XGBoost", "Negative value added"), bty = "n", pch = c(15,16,17) , col=colors_in , text.col = "black", cex=0.7, pt.cex=1.5)
```
Save the radar chart in table format
```{r}
data_plot = t(data_plot)
write.csv(data_plot[,3:5], "C:/Users/fabio/OneDrive/Desktop/zombie_nace_NEG_VA.csv")
```





## Additional analysis: ICR and Negative value-added missingness across Italy

```{r}
data_count1 <- aggregate(cbind(ICR_failure_2008, ICR_failure_2009, ICR_failure_2010, 
                               ICR_failure_2011, ICR_failure_2012, ICR_failure_2013, 
                               ICR_failure_2014, ICR_failure_2015, ICR_failure_2016) ~ zone,                        # Count NA by group`
                         train,
                         function(x) { sum(is.na(x)) / length(x) },
                         na.action = NULL)

rownames(data_count1) <- c("Islands", "Center", "North", "South")

rowMeans(data_count1[, 2:ncol(data_count1)]) * 100
```


```{r}
data_count1 <- aggregate(cbind(NEG_VA_2008, NEG_VA_2009, NEG_VA_2010, 
                               NEG_VA_2011, NEG_VA_2012, NEG_VA_2013, 
                               NEG_VA_2014, NEG_VA_2015, NEG_VA_2016) ~ zone,                        # Count NA by group`
                         train,
                         function(x) { sum(is.na(x)) / length(x) },
                         na.action = NULL)

rownames(data_count1) <- c("Islands", "Center", "North", "South")

rowMeans(data_count1[, 2:ncol(data_count1)]) * 100
```








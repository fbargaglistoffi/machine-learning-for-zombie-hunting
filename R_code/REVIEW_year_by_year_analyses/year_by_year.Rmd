---
title: "Year by year analysis"
author: "Fabio Incerti"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'H:\\.shortcut-targets-by-id\\1keYb51HXkcQwzkU2kBgbnguQaqUy3umX\\Zombie Hunting New Data')
```

Clean the environment
```{r}
rm(list=ls())
```

```{r warning=FALSE}
library(haven)
library(mlr)
library(xgboost)
library(data.table)
library(caret)
library(PRROC)
library(gtools)
library(sur)
library(Amelia)
library(dplyr)
library(tidyr)
library(naniar)
library(plotrix)
library(ggplot2)
source('C:/Users/fabio/OneDrive/Documenti/GitHub/machine-learning-for-zombie-hunting/R_code/REVIEW_year_by_year_analyses/functions.R')

#data_2 <- read_dta("H://.shortcut-targets-by-id//1keYb51HXkcQwzkU2kBgbnguQaqUy3umX//Zombie Hunting New Data/analysis_data_indicators.dta")
```

Upload data
```{r}
data <- read_dta("analysis_data_indicators.dta")
```

Get Italian data
```{r}
data_italy <- data[which(data$iso=="IT"),] 
```

```{r}
names(data_italy)[names(data_italy) == 'GUO___BvD_ID_number'] <- 'guo'

data_italy$control <- ifelse(data_italy$guo=="", 0, 1)
data_italy$nace <- as.factor(data_italy$nace)
data_italy$area <- as.factor(data_italy$area)

levels(data_italy$nace) <- floor(as.numeric(levels(data_italy$nace))/100) 
```

```{r}
status_aggregated <- as.matrix(NA, nrow = nrow(data_italy), ncol = 1)
status_aggregated[which(data_italy$Status=="Active")] <- "Active" 
status_aggregated[which(data_italy$Status=="Active (default of payment)")] <- "Active" 
status_aggregated[which(data_italy$Status=="Active (insolvency proceedings)")] <- "Active" 
status_aggregated[which(data_italy$Status=="Active (rescue plan)")] <- "Active" 
status_aggregated[which(data_italy$Status=="Bankruptcy")] <- "Bankruptcy" 
status_aggregated[which(data_italy$Status=="Dissolved")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (bankruptcy)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (demerged)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (liquidation)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="Dissolved (merger or take-over)")] <- "Dissolved" 
status_aggregated[which(data_italy$Status=="In liquidation")] <- "In Liquidation" 
```


Rename incorrectly labeled variables:
- misallocated_2008_fixed -> misallocated_fixed_2008
- revenue2008 -> revenue_2008
- depr2008 -> depr_2008                                                                            )
```{r}
for (i in c(2008:2016)) {
  colnames(data_italy) <- gsub(paste("misallocated", sep="_", i, "fixed"), paste("misallocated", sep="_", "fixed", i), colnames(data_italy))
  colnames(data_italy) <- gsub(paste("revenue", sep="", i), paste("revenue", sep="_", i), colnames(data_italy))
  colnames(data_italy) <- gsub(paste("depr", sep="", i), paste("depr", sep="_", i), colnames(data_italy))
}
```


Aggregated failure status composition
```{r}
print("Number of failures per category")
table(as.factor(status_aggregated[status_aggregated!="Active"]))
print("Failures share per category")
percent.table(x = as.factor(status_aggregated[status_aggregated!="Active"]), y = NULL)
```

Remove firms in liquidation: NO, WE KEEP THEM 
```{r}
print("Number of firms in liquidation")
nrow(data_italy[which(data_italy$Status == "In liquidation"), ])

# Save firms in liquidation
data_italy_liq = data_italy[which(data_italy$Status == "In liquidation"), ]
# Save bankruptcy and dissolved firms
data_italy_non_liq = data_italy[which(data_italy$Status == "Bankruptcy" | data_italy$Status == "Dissolved"), ] 

# Uncomment to drop firms in liquidation
#data_italy = data_italy[which(data_italy$Status != "In liquidation"), ]
```



Construct yearly failure Indicator
```{r}
data_italy <- failure_yearly(data_italy)
```


Get failures over time without liquidations (i.e. bankruptcies and dissolved firms)
```{r}
#colSums(data_italy[which(colnames(data_italy) == "failure_2009"):which(colnames(data_italy) == "failure_2017")], na.rm = TRUE)
```

Liquidations over time
```{r}
data_italy_liq <- failure_yearly(data_italy_liq)
colSums(data_italy_liq[which(colnames(data_italy_liq) == "failure_2009"):which(colnames(data_italy_liq) == "failure_2017")], na.rm = TRUE)
```



Define the relevant predictors
```{r}
# Years 2008-2016
lagged_predictors <- c("shareholders_funds",
                "added_value", "cash_flow", "ebitda",
                "fin_rev", "liquidity_ratio", "total_assets",
                "depr", "long_term_debt", "employees",
                "materials", "loans", "wage_bill", "tfp_acf",
                "fixed_assets", "tax", "current_liabilities",
                "current_assets", "fin_expenses", "int_paid",
                "solvency_ratio", "net_income", "revenue",
                "capital_intensity", "fin_cons",
                "ICR_failure", "NEG_VA",
                "real_SA", "profitability", "misallocated_fixed",
                "financial_sustainability", "liquidity_return",
                "int_fixed_assets")
dyn_predictors <- paste(lagged_predictors, sep = "_", 2008, collapse = NULL)
for (i in c(2009:2016)) {
                    dyn_predictors <- append(dyn_predictors, paste(lagged_predictors, sep = "_", i, collapse = NULL) ) 
  
}


# Years 2009-2016
outside = c("inv", "interest_diff")
dyn_outside <- paste(outside, sep = "_", 2009, collapse = NULL)
for (i in c(2010:2016)) {
                    dyn_outside <- append(dyn_outside, paste(outside, sep = "_", i, collapse = NULL) ) 
  
}

# Time-invariant
predictors <- c("control", "nace", "consdummy", "area", "dummy_patents", "dummy_trademark")
```


# Missingness patterns

```{r}
# Define new labels for ggplot
new.names <-  c(control = "Corporate control",
                miss = "Missingness",
                net_income = "Net income",
                employees = "Employees",
                tax = "Tax",
                real_SA = "Size-Age",
                fixed_assets = "Fixed assets",
                shareholders_funds = "Shareholders funds",
                revenue = "Revenues",
                tfp_acf = "TFP",
                inv = "Investement indicator",
                current_liabilities = "Current liabilities",
                current_assets = "Current assets",
                wage_bill = "Cost of employees",
                solvency_ratio = "Solvency ratio",
                total_assets = "Total assets",
                int_fixed_assets = "Intangible fixed assets",
                fin_rev = "Financial revenues",
                nace = "NACE rev. 2",
                materials = "Material costs",
                liquidity_return = "Liquidity return",
                fin_expenses = "Financial expenses",
                depr = "Depreciation",
                area = "Area",
                liquidity_ratio = "Liquidity ratio",
                ebitda = "EBITDA",
                int_paid = "Interest payments",
                financial_sustainability = "Financial sustainability",
                capital_intensity = "Capital intensity",
                loans = "Loans",
                added_value = "Added value",
                long_term_debt = "Long-term debt",
                cash_flow = "Cash flow",
                fin_cons = "Financial Constraints",
                interest_diff = "Benchmark interest difference",
                dummy_trademark = "Dummy trademarks",
                dummy_patents = "Dummy patents",
                misallocated_fixed = "Financial misallocation",
                ICR_failure = "ICR",
                profitability = "Profitability",
                consdummy = "Dummy consolidated accounts",
                NEG_VA = "Negative value added")
```


Non-failing firms (N=287587)
```{r}
# Replace "_" with "." for reshaping data from wide to long format (e.g. shareholders_funds_2008 --> shareholders_funds.2009)
prova = data_italy[ which(status_aggregated=="Active"), c("id", dyn_predictors)]
for (i in c(2008:2016)) {
  colnames(prova) = gsub(paste("_", sep = "", i), paste(".", sep = "", i), colnames(prova))
}

# Reshape data from wide to long format
prova_2 = reshape(data = as.data.frame(prova), 
                  varying = 2:length(prova),
                  timevar = "Year",
                  idvar = "id",
                  direction = "long",
                  sep = ".")

# Plot missingness patterns over years
gg_miss_fct(prova_2[2:length(prova_2)], Year) + 
  labs(title = "Non-failing firms", 
           x = "Years", 
           y = "Variables") +
  scale_x_discrete(limits = c(2008:2016)) +
  scale_y_discrete(labels = new.names) +
  scale_fill_continuous(type = "viridis", name = "% Missing") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.y = element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank()) 

# Uncomment for table information
#sapply(data_italy[c("id", dyn_predictors)], function(x) sum(is.na(x))/nrow(data_italy))
```


Firms in liquidation
```{r}
# Replace "_" with "." for reshaping data from wide to long format (e.g. shareholders_funds_2008 --> shareholders_funds.2009)
prova = data_italy_liq[c("id", dyn_predictors)]
for (i in c(2008:2016)) {
  colnames(prova) = gsub(paste("_", sep = "", i), paste(".", sep = "", i), colnames(prova))
}

# Reshape data from wide to long format
prova_2 = reshape(data = as.data.frame(prova), 
                  varying = 2:length(prova),
                  timevar = "Year",
                  idvar = "id",
                  direction = "long",
                  sep = ".")

# Plot missingness patterns over years
gg_miss_fct(prova_2[2:length(prova_2)], Year) + 
  labs(title = "Liquidations", 
           x = "Years", 
           y = "Variables") +
  scale_x_discrete(limits = c(2008:2016)) +
  scale_y_discrete(labels = new.names) +
  scale_fill_continuous(type = "viridis", name = "% Missing") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.y = element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank()) 

# Uncomment for table information
#sapply(data_italy_liq[c("id", dyn_predictors)], function(x) sum(is.na(x))/nrow(data_italy_liq))
```


Bankruptcies and dissolved firms
```{r}
# Replace "_" with "." for reshaping data from wide to long format (e.g. shareholders_funds_2008 --> shareholders_funds.2009)
prova = data_italy_non_liq[c("id", dyn_predictors)]
for (i in c(2008:2016)) {
  colnames(prova) = gsub(paste("_", sep = "", i), paste(".", sep = "", i), colnames(prova))
}

# Reshape data from wide to long format
prova_2 = reshape(data = as.data.frame(prova), 
                  varying = 2:length(prova),
                  timevar = "Year",
                  idvar = "id",
                  direction = "long",
                  sep = ".")

# Plot missingness patterns over years
gg_miss_fct(prova_2[2:length(prova_2)], Year) + 
  labs(title = "Bankruptcies and other dissolved firms", 
           x = "Years", 
           y = "Variables" ) +
  scale_x_discrete(limits = c(2008:2016)) +
  scale_y_discrete(labels = new.names) +
  scale_fill_continuous(type = "viridis", name = "% Missing") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.y = element_text(size=7),
        axis.title.y=element_blank(),
        axis.title.x=element_blank()) 

# Uncomment for table information
#sapply(data_italy_non_liq[c("id", dyn_predictors)], function(x) sum(is.na(x))/nrow(data_italy_non_liq))
```









#
Analysis for Italian Firms (2009-2017)

XGBoost requires one-hot encoding of the categorical predictors (i.e. nace and area)
```{r}
# Get one-hot encoded data.frame
dummy = dummyVars(" ~ nace + area", data = data_italy)
temp = data.frame(predict(dummy, newdata = data_italy))

# Add one-hot encoded variables to the original data
train = cbind(data_italy, temp)

# Drop the original nace and area variables
train = within(train, rm("nace", "area"))
```


Year-by-year analysis: train XGBoost with missing values for the years 2009-2017
```{r}
years <- c(2008:2016)
prob <- matrix(NA, ncol = length(years), nrow = nrow(train))

for (i in (years)){
    
    if (i == 2008) {

        # Subset data by year
        data_model <- train[which(!is.na(train[,paste("failure", sep = "_", i + 1)])),]
        
        # Get predictors matrix (with no "inv" and "interest_diff" for 2008)
        X <- as.data.frame(cbind(data_model[paste(lagged_predictors, sep = "_", i)], 
                                 data_model[grep("nace", colnames(data_model))], 
                                 data_model[grep("area", colnames(data_model))]))
        
        # XGBoost requires matrix format for the predictor matrix
        X <- matrix(unlist(X), ncol = length(X), nrow = nrow(X))
        
        # Get label vector
        y <- as.vector(data_model[,paste("failure", sep = "_", i + 1)])
        
        # Run XGboost
        set.seed(2020)
        xgboost <- xgboost(data = X, label = y, objective = "binary:logistic", 
                           nrounds = 100, 
                           verbose = 0)
        
        # Get fitted values
        prob[which(!is.na(train[,paste("failure", sep = "_", i + 1)])), which(years==i)] <-  predict(xgboost, newdata = X, type = "response" ) 
    
    
    } else {
      
        # Subset data by year
        data_model <- train[which(!is.na(train[,paste("failure", sep = "_", i + 1)])),]
        
        # Get predictors matrix (from 2009 onwards we now have "inv" and "interest_diff") and the one-hot encoded predictors
        X <- as.data.frame(cbind(data_model[,paste(lagged_predictors, sep = "_", i)], 
                                 data_model[,paste(outside, sep = "_", i)],
                                 data_model[grep("nace", colnames(data_model))], 
                                 data_model[grep("area", colnames(data_model))]))
        
        # XGBoost requires matrix format for the predictor matrix
        X <- matrix(unlist(X), ncol = length(X), nrow = nrow(X))
        
        # Get label vector
        y <- as.vector(data_model[,paste("failure", sep = "_", i + 1)])
        
        # Run XGBoost
        set.seed(2020)
        xgboost <- xgboost(data = X, label = y, objective = "binary:logistic", nrounds = 100, verbose = 0)
        
        # Get fitted values
        prob[which(!is.na(train[,paste("failure", sep = "_", i + 1)])), which(years==i)] <-  predict(xgboost, newdata = X, type = "response" )   
      
      
    }
}

# Save Results
train$prob_2009 <- prob[,1]
train$prob_2010 <- prob[,2]
train$prob_2011 <- prob[,3]
train$prob_2012 <- prob[,4]
train$prob_2013 <- prob[,5]
train$prob_2014 <- prob[,6]
train$prob_2015 <- prob[,7]
train$prob_2016 <- prob[,8]
train$prob_2017 <- prob[,9]
```

Save data augmented of default probabilities
```{r}
# Rename one-hot encoded variables because "." is considered an illegal character
colnames(train) = gsub("nace.", "nace_", colnames(train))
colnames(train) = gsub("area.", "area_", colnames(train))

# Save new data
# Without liquidations: write_dta(train, "C:/Users/fabio/OneDrive/Documenti/GitHub/machine-learning-for-zombie-hunting/R_code/REVIEW_year_by_year_analyses/data_year_by_year.dta")
write_dta(train, "C:/Users/fabio/OneDrive/Documenti/GitHub/machine-learning-for-zombie-hunting/R_code/REVIEW_year_by_year_analyses/data_liq_year_by_year.dta")
```

Load previously saved data
```{r}
#train <- read_dta("C:/Users/fabio/OneDrive/Documenti/GitHub/machine-learning-for-zombie-hunting/R_code/REVIEW_year_by_year_analyses/data_year_by_year.dta")
train <- read_dta("C:/Users/fabio/OneDrive/Documenti/GitHub/machine-learning-for-zombie-hunting/R_code/REVIEW_year_by_year_analyses/data_liq_year_by_year.dta")
```


# Statistics: in-sample performance over years
```{r}

years <- c(2014:2017)
for (i in (years)) {

index_failure <- paste("failure", i, sep = "_")
index_prob <- paste("prob", i, sep = "_")


data_model <- train[which(!is.na(train[ , eval(index_failure)])), ]
data_model[[eval(index_failure)]] <- as.factor(data_model[[eval(index_failure)]])

# Separating the predictions of firms who actually failed in year i from those who don't
fg.xgboost<-data_model[[eval(index_prob)]][ data_model[[eval(index_failure)]]==1 ] 
bg.xgboost<-data_model[[eval(index_prob)]][ data_model[[eval(index_failure)]]==0 ] 


# ROC curve
roc_xgboost<-roc.curve(scores.class0 = fg.xgboost,
                    scores.class1 = bg.xgboost,
                    curve = T)
#jpeg(file="roc_xgboost.jpg")
#plot(roc_xgboost)
#dev.off()


# Area under the PR curve
pr_xgboost<-pr.curve(scores.class0 = fg.xgboost,
                  scores.class1 = bg.xgboost,
                  curve = T)
# jpeg(file="pr_xgboost.jpg")
# plot(pr_xgboost)
# dev.off()


# Approccio data-driven: predico che fallisci se stai sopra alla mediana o ad un certo percentile
# Approccio "teorico": fisso una soglia arbitraria (es. 0.03)
#  
fitted.xgboost <- ifelse(data_model[[eval(index_prob)]]> quantile(data_model[[eval(index_prob)]], 
                                                               probs = c(0.5), na.rm = TRUE), 1, 0)
fitted.xgboost <- as.factor(fitted.xgboost)

# F1-Score and BACC
conf_matrix <- confusionMatrix(as.factor(fitted.xgboost), as.factor(data_model[[eval(index_failure)]]), mode = "everything", positive="1") 

f1_xgboost <- conf_matrix[[4]][["F1"]]
balanced_accuracy_xgboost <- conf_matrix[[4]][["Balanced Accuracy"]]

# Rsquared 
accuracy_xgboost <- as.data.frame(rbind(postResample(as.double(fitted.xgboost), data_model[[eval(index_prob)]])))


# Append all statistics
if(which(years==i)==1) {
     xgboost_fit <- as.data.frame(cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost,
                                      balanced_accuracy_xgboost, accuracy_xgboost$Rsquared))
     
}
else{
     xgboost_fit <- rbind(xgboost_fit, cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost, 
                            balanced_accuracy_xgboost, accuracy_xgboost$Rsquared  ) )
      
}
}
colnames(xgboost_fit) <- c("Year", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared")
xgboost_fit
```





# Out-of-range values imputation: check yearly performances

```{r}
# Get one-hot encoded data.frame
dummy = dummyVars(" ~ nace + area", data = data_italy)
temp = data.frame(predict(dummy, newdata = data_italy))

# Add one-hot encoded variables to the original data
train = cbind(data_italy, temp)

# Drop the original nace and area variables
train = within(train, rm("nace", "area"))


labels = c("failure_2009", "failure_2010", "failure_2011", "failure_2012", "failure_2013",
           "failure_2014", "failure_2015", "failure_2016", "failure_2017")

temp = train[!names(train) %in% labels]
temp[is.na(temp)] = 10^20
new_train = cbind(train[labels], temp)

```

```{r}
years <- c(2008:2016)
prob <- matrix(NA, ncol = length(years), nrow = nrow(new_train))

for (i in (years)){
    
    if (i == 2008) {

        # Subset data by year
        data_model <- new_train[which(!is.na(new_train[,paste("failure", sep = "_", i + 1)])),]
        
        # Get predictors matrix (with no "inv" and "interest_diff" for 2008)
        X <- as.data.frame(cbind(data_model[paste(lagged_predictors, sep = "_", i)], 
                                 data_model[grep("nace", colnames(data_model))], 
                                 data_model[grep("area", colnames(data_model))]))
        
        # XGBoost requires matrix format for the predictor matrix
        X <- matrix(unlist(X), ncol = length(X), nrow = nrow(X))
        
        # Get label vector
        y <- as.vector(data_model[,paste("failure", sep = "_", i + 1)])
        
        # Run XGboost
        set.seed(2020)
        xgboost <- xgboost(data = X, label = y, objective = "binary:logistic", nrounds = 100, verbose = 0)
        
        # Get fitted values
        prob[which(!is.na(new_train[,paste("failure", sep = "_", i + 1)])), which(years==i)] <-  predict(xgboost, newdata = X, type = "response" ) 
    
    
    } else {
      
        # Subset data by year
        data_model <- new_train[which(!is.na(new_train[,paste("failure", sep = "_", i + 1)])),]
        
        # Get predictors matrix (from 2009 onwards we now have "inv" and "interest_diff") and the one-hot encoded predictors
        X <- as.data.frame(cbind(data_model[,paste(lagged_predictors, sep = "_", i)], 
                                 data_model[,paste(outside, sep = "_", i)],
                                 data_model[grep("nace", colnames(data_model))], 
                                 data_model[grep("area", colnames(data_model))]))
        
        # XGBoost requires matrix format for the predictor matrix
        X <- matrix(unlist(X), ncol = length(X), nrow = nrow(X))
        
        # Get label vector
        y <- as.vector(data_model[,paste("failure", sep = "_", i + 1)])
        
        # Run XGBoost
        set.seed(2020)
        xgboost <- xgboost(data = X, label = y, objective = "binary:logistic", nrounds = 100, verbose = 0)
        
        # Get fitted values
        prob[which(!is.na(new_train[,paste("failure", sep = "_", i + 1)])), which(years==i)] <-  predict(xgboost, newdata = X, type = "response" )   
      
      
    }
}

# Save Results
new_train$prob_2009 <- prob[,1]
new_train$prob_2010 <- prob[,2]
new_train$prob_2011 <- prob[,3]
new_train$prob_2012 <- prob[,4]
new_train$prob_2013 <- prob[,5]
new_train$prob_2014 <- prob[,6]
new_train$prob_2015 <- prob[,7]
new_train$prob_2016 <- prob[,8]
new_train$prob_2017 <- prob[,9]
```


Statistics
```{r}
years <- c(2014:2017)
for (i in (years)) {

index_failure <- paste("failure", i, sep = "_")
index_prob <- paste("prob", i, sep = "_")


data_model <- new_train[which(!is.na(new_train[ , eval(index_failure)])), ]
data_model[[eval(index_failure)]] <- as.factor(data_model[[eval(index_failure)]])

# Separating the predictions of firms who actually failed in year i from those who don't
fg.xgboost<-data_model[[eval(index_prob)]][ data_model[[eval(index_failure)]]==1 ] 
bg.xgboost<-data_model[[eval(index_prob)]][ data_model[[eval(index_failure)]]==0 ] 


# ROC curve
roc_xgboost<-roc.curve(scores.class0 = fg.xgboost,
                    scores.class1 = bg.xgboost,
                    curve = T)
#jpeg(file="roc_xgboost.jpg")
#plot(roc_xgboost)
#dev.off()


# Area under the PR curve
pr_xgboost<-pr.curve(scores.class0 = fg.xgboost,
                  scores.class1 = bg.xgboost,
                  curve = T)
# jpeg(file="pr_xgboost.jpg")
# plot(pr_xgboost)
# dev.off()


# Approccio data-driven: predico che fallisci se stai sopra alla mediana o ad un certo percentile
# Approccio "teorico": fisso una soglia arbitraria (es. 0.03)
#  
fitted.xgboost <- ifelse(data_model[[eval(index_prob)]]> quantile(data_model[[eval(index_prob)]], 
                                                               probs = c(0.5), na.rm = TRUE), 1, 0)
fitted.xgboost <- as.factor(fitted.xgboost)

# F1-Score and BACC
conf_matrix <- confusionMatrix(as.factor(fitted.xgboost), as.factor(data_model[[eval(index_failure)]]), mode = "everything", positive="1") 

f1_xgboost <- conf_matrix[[4]][["F1"]]
balanced_accuracy_xgboost <- conf_matrix[[4]][["Balanced Accuracy"]]

# Rsquared 
accuracy_xgboost <- as.data.frame(rbind(postResample(as.double(fitted.xgboost), data_model[[eval(index_prob)]])))


# Append all statistics
if(which(years==i)==1) {
     xgboost_fit <- as.data.frame(cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost,
                                      balanced_accuracy_xgboost, accuracy_xgboost$Rsquared))
     
}
else{
     xgboost_fit <- rbind(xgboost_fit, cbind(i, roc_xgboost$auc, pr_xgboost$auc.integral, f1_xgboost, 
                            balanced_accuracy_xgboost, accuracy_xgboost$Rsquared  ) )
      
}
}
colnames(xgboost_fit) <- c("Year", "Area under the ROC", "Area under the PR", "F1-Score", "BACC", "R squared")
xgboost_fit
```













# Figure 9: compute the share of zombie firms over time

We consider a firm as a zombie if lying above the 9th decile for three consecutive years without failing (i.e. False Positive)


Fix the quantile threshold discriminating exit and non-exit prediction (where we expect lie most zombie firms)
```{r}
cutoff = 0.90
```


Get a vectorized confusion matrix in each year by comparing:
- the threshold-based (integer) fitted values
- the actual failure indicator (label)
i.e., each non-missing observation in the fitted value is assigned a category: TP, FP, TN, FN
```{r}
years <- c(2009:2017)
train <- data.frame(train) 
categories_confusion_matrix <- data.frame(matrix(NA, ncol = length(years), nrow = nrow(train)))

for (i in years) {
  # Get non-missing observation in prob_i (and consequently in failure_i)
  index = which(!is.na(train[paste("prob", sep="_", i)]))
  
  # Transform numeric predictions into integer according to the cutoff
  fitted.complete = train[index, paste("prob", sep="_", i)]
  fitted.integer = ifelse(fitted.complete > quantile(fitted.complete, probs = cutoff, na.rm =TRUE), 1, 0)
  
  # Convert predicted and actual values into factors
  fitted.integer = as.factor(fitted.integer)
  ground_truth = as.factor(train[index, paste("failure", sep="_", i)]) 
  
  # Assign to each firm its confusion matrix category
  vectorized_conf_matrix = matrix(NA, ncol = 1, nrow = length(fitted.integer))
  vectorized_conf_matrix[which(fitted.integer == 1 & ground_truth == 0)] = "FP"    # False positives: potential ZOMBIES if resulting FP for 3 consecutive years 
  vectorized_conf_matrix[which(fitted.integer == 1 & ground_truth == 1)] = "TP"    # True positives: failed firms above the 9th decile
  vectorized_conf_matrix[which(fitted.integer == 0 & ground_truth == 1)] = "FN"    # False negatives: failed firms below the 9th decile
  vectorized_conf_matrix[which(fitted.integer == 0 & ground_truth == 0)] = "TN"    # True negatives: non-failing firms below the 9th decile
  
  # Transform into factor
  vectorized_conf_matrix = factor(vectorized_conf_matrix, levels = c("FP", "TP", "FN", "TN"))
  
  # Levels will now be encoded in categories_confusion_matrix as: 1 (FP), 2 (TP), 3 (FN), 4 (TN)
  categories_confusion_matrix[index, which(years ==i)] = vectorized_conf_matrix
}

# Rename columns
names = c()
for (i in years) {
  names = append(names, paste("cat_conf_matrix", sep = "_", i))
  
}
colnames(categories_confusion_matrix) = names
```


Identify zombie firms: FP for 3 consecutive years (we get zombies for years 2011-2017)
```{r}
years <- c(2011:2017)

zombie <- data.frame(matrix(NA, ncol = length(years), nrow = nrow(categories_confusion_matrix)))

for (i in years) {
  zombie[which(years ==i)] <- ifelse(categories_confusion_matrix[ , paste("cat_conf_matrix", sep = "_", c(i-2))] == 1 &
                                     categories_confusion_matrix[ , paste("cat_conf_matrix", sep = "_", c(i-1))] == 1 & 
                                     categories_confusion_matrix[ , paste("cat_conf_matrix", sep = "_", c(i))] == 1, 1, 0)
}

# Rename columns
names = c()
for (i in years) {
  names = append(names, paste("zombie", sep = "_", i))
  
}
colnames(zombie) = names
```


Number and share of zombie firms overtime
```{r}
print("Number of zombie firms")
sapply(zombie, function(x) sum(x == 1, na.rm = TRUE))

# percent.table mi da delle % che non sommano a 1 perché la parte rimanente sono missing values: sottostima pertanto la quota di imprese zombie
# rispetto alle imprese di cui osserviamo lo status zombie/non-zombie in un certo anno
# sapply(zombie, percent.table)

print("Share of zombie firms")
sapply(zombie, function(x) length(which(x ==1)) / length(which(!is.na(x)))   ) * 100

prova = sapply(zombie, function(x) length(which(x ==1)) / length(which(!is.na(x)))   ) * 100
```


Plot the share of zombie firms (FP for three consectuvie years based on 0.9 cutoff) against GDP 
```{r}
zombie_share <- prova
gdp_growth <- c(0.707, -2.981, -1.841, -0.005, 0.778, 1.280, 1.716) # Data from World Bank (https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?end=2018&locations=IT&start=2011)
unemployment <- c(8.4, 10.7, 12.1, 12.7, 11.9, 11.7, 11.2) # Data from Statista (https://www.statista.com/statistics/531010/unemployment-rate-italy/)
years <- c(seq(2011,2017,1))


twoord.plot(lx = years, ly = zombie_share,
            rx = years, ry = gdp_growth,
            type= c("l", "l"),
            lwd = 3,
            yaxt = 'n',
            lylim = range(zombie_share) + c(-0.5,0.5),
            rylim = range(gdp_growth) + c(-0.5,0.5),
            lcol = "blue",
            xlab = "Years",
            ylab = "Zombie rate",
            rylab = "GDP growth rate",
            #main = "Share of Zombie Firms and GDP growth",
            do.first="plot_bg();grid(col=\"white\",lty=1)")

twoord.plot(lx = years, ly = zombie_share,
            rx = years, ry = unemployment,
            type= c("l", "l"),
            lwd = 3, 
            yaxt = 'n',
            lylim = range(zombie_share) + c(-0.5,0.5),
            rylim = range(unemployment) + c(-0.5,0.5),
            lcol = 4,
            rcol="coral4",
            xlab = "Years",
            ylab = "Zombie rate",
            rylab = "Unemployment rate rate",
            #main = "Share of Zombie Firms and Unemployment Rate",
            do.first="plot_bg();grid(col=\"white\",lty=1)")
```



---
title: "Year by Year False Positives"
author: "Falco J. Bargagli Stoffi"
date: "2/12/2018"
output:
  pdf_document:
    keep_tex: true
---

# Why not just using ICR?

In this section of the Appendix we check what is the comparative advantage of using our Machine-Learning-based definition of Persistently Distressed Firms (PDF) rather than other commonly used deterministic indicators (i.e., Interest Covarage Ratio [ICR], negative added value). 
\par The first and more obvious advantage is that the PDF indicator can be constructed even in the presence of missing data. As we saw in the Section of the paper about missing data, there are significant missingness patterns in the two variables that compose the ICR (EBIT and Interest Paid).
\par The second advantage is that, while ICR entails a deterministic definition of zombies, PDF represents a probabilistic based definition that can be tuned to embody different "likelihoods" of being zombie.
\par Beside these clear advantages, there are more subtle benefits that can be obtained by using PDF instead of ICR.
To highlight these advantages we focus on a particular dimension of our PDF definition. To be a PDF a firm needs to be at high risk of failure (but not failing) for three consecutive years. 
Our PDF definition focuses on those firms that "are predicted to fail, but are surviving" (following the ML literature we could call them \textit{false positives}). 
Hence, a good indicator of "\textit{zombieness}" should be able to discriminate between \textit{false positives} and \textit{true positives}.
\par In this spirit, we develop a two stage algorithm to highlight which are the best predictors of \textit{false positives}.
In the first stage, we train a Logit model to predict the probability of failure:
\begin{equation}
 f_{LOGIT}(x) = \hat{p}_i(Y_i = 1 | X_i = x),
\end{equation}
and to obtain the fitted values $\hat{y}_i$.
In the second stage, we fit a LASSO just on those observations with $\hat{y}_i=1$ (\textit{positives}) to get the most important predictors (namely, those with non-zero coefficient) for the \textit{false positives}.
\par In the following code chuncks the implementation of this "two-stage" algorithm.

## Load Packages and Data

In this chunk we load the packages and the data used for the analysis, and we split the overall sample in a \textit{training} and a \textit{test} set.

```{r results = 'hide', include = TRUE, message = FALSE, warning = FALSE}
options(java.parameters = "-Xmx50g")
library(rJava)
library(bartMachine)
library(haven)
library(plyr)
library(dplyr)
library(PRROC)
library(caret)
library(glmnet)

#Upload dati
setwd("G:\\Il mio Drive\\Research\\Italian Firms\\Zombie Hunting New Data")

#LOAD DATA

data <- read_dta("data_italy.dta")

#OMIITED DATA
#2016
myvariables <- c("tfp_acf_2016", "dummy_patents", "dummy_trademark", "consdummy", 
                "nace_2", "fin_rev_2016", "int_paid_2016", "ebitda_2016", "cash_flow_2016",
                "revenue2016", "total_assets_2016",  "long_term_debt_2016", "employees_2016",
                "added_value_2016", "materials_2016", "wage_bill_2016", "loans_2016" , 
                "int_fixed_assets_2016", "fixed_assets_2016", "control",  "depr2016", 
                "current_liabilities_2016",  "liquidity_ratio_2016",  "solvency_ratio_2016", 
                "current_assets_2016", "fin_expenses_2016", "net_income_2016",  "fin_cons100_2016" , 
                "inv_2016" , "real_SA_2016",  "shareholders_funds_2016" , "NEG_VA_2016" , 
                "ICR_failure_2016" , "profitability_2016" , "misallocated_2016_fixed",  
                "interest_diff_2016", "failure_2017")

myvariables <- c("iso", "tfp_acf",  "Number_of_patents", "Number_of_trademarks",
                 "consdummy", "control", "failure", "nace_2","fin_rev", "int_paid",
                 "ebitda", "cash_flow", "depr", "revenue", "total_assets", 
                 "long_term_debt", "employees", "added_value", "materials", "wage_bill",
                 "loans" , "int_fixed_assets", "fixed_assets", "current_liabilities",
                 "liquidity_ratio",  "solvency_ratio", "current_assets", "fin_expenses",
                 "net_income", "fin_cons", "fin_cons100", "inv",  "ICR_t", "time",
                 "real_SA", "shareholders_funds", "NEG_VA", "ICR_failure", "profitability",
                 "misallocated_fixed","interest_diff")
#capital_intensity, labour_product,retained_earnings, firm_value excluded: highly missing
dati <- na.omit(data[myvariables])
predictors <- c("tfp_acf_2016", "dummy_patents", "dummy_trademark", "consdummy", 
                "nace_2", "fin_rev_2016", "int_paid_2016", "ebitda_2016", "cash_flow_2016",
                "revenue2016", "total_assets_2016",  "long_term_debt_2016", "employees_2016",
                "added_value_2016", "materials_2016", "wage_bill_2016", "loans_2016" , 
                "int_fixed_assets_2016", "fixed_assets_2016", "control",  "depr2016", 
                "current_liabilities_2016",  "liquidity_ratio_2016",  "solvency_ratio_2016", 
                "current_assets_2016", "fin_expenses_2016", "net_income_2016",  "fin_cons100_2016" , 
                "inv_2016" , "real_SA_2016",  "shareholders_funds_2016" , "NEG_VA_2016" , 
                "ICR_failure_2016" , "profitability_2016" , "misallocated_2016_fixed",  
                "interest_diff_2016")

### Define samples
set.seed(123)
train_sample <- sample(seq_len(nrow(dati)), size = nrow(dati)*0.5) 
train <- as.data.frame(dati[train_sample,])
test <- as.data.frame(dati[-train_sample,])
```

In this chunck we contruct the LOGIT model, we get the predicted probabilities and the confusion matrix.
We use as a threshold 0.3 (namely,  $\hat{p}_i(Y_i = 1 | X_i = x)>0.3 \rightarrow \hat{y}_i=1$), however this threshold can be moved up to 0.5 without any meaninful change. 

```{r include = TRUE, message = FALSE, warning = FALSE}
log = glm(formula = train$failure_2017 ~ ., family = binomial, data = train)
prob_pred = predict(log, type = 'response', newdata = test)
prediction <- as.numeric(prob_pred > 0.3) # change up to 0.5
cmlog=table(test$failure,prediction)
cmlog
```

In this chunck we use a LASSO model to select the variables with the highest predictive power for \textit{false positives}.

```{r}
test$prediction <- prediction
positive <- test[which(test$prediction==1),]
positive$control <- as.numeric(as.factor(positive$control))
x<-as.matrix(positive[, c("tfp_acf_2016", "dummy_patents", "dummy_trademark", "consdummy", 
                "nace_2", "fin_rev_2016", "int_paid_2016", "ebitda_2016", "cash_flow_2016",
                "revenue2016", "total_assets_2016",  "long_term_debt_2016", "employees_2016",
                "added_value_2016", "materials_2016", "wage_bill_2016", "loans_2016" , 
                "int_fixed_assets_2016", "fixed_assets_2016", "control",  "depr2016", 
                "current_liabilities_2016",  "liquidity_ratio_2016",  "solvency_ratio_2016", 
                "current_assets_2016", "fin_expenses_2016", "net_income_2016",  "fin_cons100_2016" , 
                "inv_2016" , "real_SA_2016",  "shareholders_funds_2016" , "NEG_VA_2016" , 
                "ICR_failure_2016" , "profitability_2016" , "misallocated_2016_fixed",  
                "interest_diff_2016")])
y <- as.numeric(positive$prediction==1 & positive$failure==0)

mod <- cv.glmnet(x , y, alpha=1, intercept=FALSE)
as.matrix(coef(mod, mod$lambda.1se))
row.names(as.matrix(coef(mod, mod$lambda.1se)))[which(as.matrix(coef(mod, mod$lambda.1se))!=0)]
```

The indicator that seems to have the best disciminative power are the productivity level, the consolidated variable and the zombie indicator from Schivardi, Sette and Tabellini (2017) (\textit{misallocated\_fixed}). Hence, ICR is not among these indicators meaning that ICR alone can't catch the component of "resistence" among highly distressed firms that makes them zombies.